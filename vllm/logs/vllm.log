./scripts/serve.sh: line 77: vllm: command not found
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:01 [utils.py:325] 
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:01 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:01 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:01 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:01 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:01 [utils.py:325] 
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:01 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'max_num_batched_tokens': 16384}
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:10 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:10 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:10 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=16384.
[0;36m(APIServer pid=927318)[0;0m INFO 02-10 11:02:10 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=927318)[0;0m WARNING 02-10 11:02:10 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=927318)[0;0m WARNING 02-10 11:02:10 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=927549)[0;0m INFO 02-10 11:02:20 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=927549)[0;0m WARNING 02-10 11:02:20 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=927549)[0;0m INFO 02-10 11:02:22 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:50171 backend=nccl
[0;36m(EngineCore_DP0 pid=927549)[0;0m INFO 02-10 11:02:22 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=927549)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=927549)[0;0m INFO 02-10 11:02:34 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(EngineCore_DP0 pid=927549)[0;0m INFO 02-10 11:02:34 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=927549)[0;0m INFO 02-10 11:02:34 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=927549)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=927549)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=927549)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=927549)[0;0m INFO 02-10 11:02:38 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [gpu_model_runner.py:4128] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 190.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     raise e
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     + [
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]       ^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 244, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.self_attn = Gemma3Attention(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                      ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 142, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.qkv_proj = QKVParallelLinear(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]                     ^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 981, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     data=torch.empty(
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m ERROR 02-10 11:02:39 [core.py:946] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 190.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=927549)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=927549)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=927549)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=927549)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=927549)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=927549)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=927549)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=927549)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=927549)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=927549)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=927549)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=927549)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=927549)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=927549)[0;0m                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=927549)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=927549)[0;0m     + [
[0;36m(EngineCore_DP0 pid=927549)[0;0m       ^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=927549)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=927549)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=927549)[0;0m     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=927549)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 244, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.self_attn = Gemma3Attention(
[0;36m(EngineCore_DP0 pid=927549)[0;0m                      ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 142, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.qkv_proj = QKVParallelLinear(
[0;36m(EngineCore_DP0 pid=927549)[0;0m                     ^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 981, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=927549)[0;0m     data=torch.empty(
[0;36m(EngineCore_DP0 pid=927549)[0;0m          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=927549)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=927549)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=927549)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 190.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W210 11:02:40.788468257 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=927318)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=927318)[0;0m     sys.exit(main())
[0;36m(APIServer pid=927318)[0;0m              ^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=927318)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=927318)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=927318)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=927318)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=927318)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=927318)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=927318)[0;0m     return await main
[0;36m(APIServer pid=927318)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=927318)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=927318)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=927318)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=927318)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=927318)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=927318)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=927318)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=927318)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=927318)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=927318)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=927318)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=927318)[0;0m     return cls(
[0;36m(APIServer pid=927318)[0;0m            ^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=927318)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=927318)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=927318)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=927318)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=927318)[0;0m     super().__init__(
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=927318)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=927318)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=927318)[0;0m     next(self.gen)
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=927318)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=927318)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=927318)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=927318)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=928261)[0;0m INFO 02-10 11:12:25 [utils.py:325] 
[0;36m(APIServer pid=928261)[0;0m INFO 02-10 11:12:25 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=928261)[0;0m INFO 02-10 11:12:25 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=928261)[0;0m INFO 02-10 11:12:25 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=928261)[0;0m INFO 02-10 11:12:25 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=928261)[0;0m INFO 02-10 11:12:25 [utils.py:325] 
[0;36m(APIServer pid=928261)[0;0m INFO 02-10 11:12:25 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'dtype': 'float16', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192}
[0;36m(APIServer pid=928261)[0;0m INFO 02-10 11:12:25 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=928261)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=928261)[0;0m     sys.exit(main())
[0;36m(APIServer pid=928261)[0;0m              ^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=928261)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=928261)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=928261)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=928261)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=928261)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=928261)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=928261)[0;0m     return await main
[0;36m(APIServer pid=928261)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=928261)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=928261)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=928261)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=928261)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=928261)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=928261)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=928261)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=928261)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=928261)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=928261)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[0;36m(APIServer pid=928261)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 1374, in create_engine_config
[0;36m(APIServer pid=928261)[0;0m     model_config = self.create_model_config()
[0;36m(APIServer pid=928261)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 1228, in create_model_config
[0;36m(APIServer pid=928261)[0;0m     return ModelConfig(
[0;36m(APIServer pid=928261)[0;0m            ^^^^^^^^^^^^
[0;36m(APIServer pid=928261)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[0;36m(APIServer pid=928261)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[0;36m(APIServer pid=928261)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[0;36m(APIServer pid=928261)[0;0m   Value error, The model type 'gemma3' does not support float16. Reason: Numerical instability. Please use bfloat16 or float32 instead. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[0;36m(APIServer pid=928261)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [utils.py:325] 
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [utils.py:325] 
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192}
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:46 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:47 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=928423)[0;0m INFO 02-10 11:12:47 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=928423)[0;0m WARNING 02-10 11:12:47 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=928423)[0;0m WARNING 02-10 11:12:47 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=928538)[0;0m INFO 02-10 11:12:58 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=928538)[0;0m WARNING 02-10 11:12:58 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 11:13:06 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 11:13:06 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 11:13:08 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:47291 backend=nccl
INFO 02-10 11:13:08 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:47291 backend=nccl
INFO 02-10 11:13:08 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 11:13:08 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 11:13:08 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 11:13:08 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 11:13:08 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 11:13:08 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 11:13:08 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 11:13:08 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 11:13:08 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=928538)[0;0m ERROR 02-10 11:13:16 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=928538)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=928538)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=928538)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=928538)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=928538)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=928538)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=928538)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=928538)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=928538)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=928538)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=928538)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=928538)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=928538)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=928538)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=928538)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=928538)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=928538)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=928423)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=928423)[0;0m     sys.exit(main())
[0;36m(APIServer pid=928423)[0;0m              ^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=928423)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=928423)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=928423)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=928423)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=928423)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=928423)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=928423)[0;0m     return await main
[0;36m(APIServer pid=928423)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=928423)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=928423)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=928423)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=928423)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=928423)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=928423)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=928423)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=928423)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=928423)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=928423)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=928423)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=928423)[0;0m     return cls(
[0;36m(APIServer pid=928423)[0;0m            ^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=928423)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=928423)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=928423)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=928423)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=928423)[0;0m     super().__init__(
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=928423)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=928423)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=928423)[0;0m     next(self.gen)
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=928423)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=928423)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=928423)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=928423)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [utils.py:325] 
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [utils.py:325] 
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 8192}
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=928867)[0;0m INFO 02-10 11:14:35 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=928867)[0;0m WARNING 02-10 11:14:35 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=928867)[0;0m WARNING 02-10 11:14:35 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=929006)[0;0m INFO 02-10 11:14:45 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=929006)[0;0m WARNING 02-10 11:14:45 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 11:14:53 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 11:14:53 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 11:14:55 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:53295 backend=nccl
INFO 02-10 11:14:55 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:53295 backend=nccl
INFO 02-10 11:14:55 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 11:14:55 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 11:14:55 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 11:14:55 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 11:14:55 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 11:14:55 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 11:14:55 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 11:14:55 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 11:14:55 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=929006)[0;0m ERROR 02-10 11:15:05 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=929006)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=929006)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=929006)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=929006)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=929006)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=929006)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=929006)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=929006)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=929006)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=929006)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=929006)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=929006)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=929006)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=929006)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929006)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=929006)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=929006)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=928867)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=928867)[0;0m     sys.exit(main())
[0;36m(APIServer pid=928867)[0;0m              ^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=928867)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=928867)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=928867)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=928867)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=928867)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=928867)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=928867)[0;0m     return await main
[0;36m(APIServer pid=928867)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=928867)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=928867)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=928867)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=928867)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=928867)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=928867)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=928867)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=928867)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=928867)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=928867)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=928867)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=928867)[0;0m     return cls(
[0;36m(APIServer pid=928867)[0;0m            ^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=928867)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=928867)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=928867)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=928867)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=928867)[0;0m     super().__init__(
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=928867)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=928867)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=928867)[0;0m     next(self.gen)
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=928867)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=928867)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=928867)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=928867)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [utils.py:325] 
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [utils.py:325] 
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.6, 'max_num_batched_tokens': 8192}
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:26 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:27 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=929349)[0;0m INFO 02-10 11:16:27 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=929349)[0;0m WARNING 02-10 11:16:27 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=929349)[0;0m WARNING 02-10 11:16:27 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=929464)[0;0m INFO 02-10 11:16:36 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=929464)[0;0m WARNING 02-10 11:16:36 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 11:16:43 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 11:16:43 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 11:16:47 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:36271 backend=nccl
INFO 02-10 11:16:47 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:36271 backend=nccl
INFO 02-10 11:16:47 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 11:16:47 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 11:16:47 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 11:16:47 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 11:16:47 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 11:16:47 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 11:16:47 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 11:16:47 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 11:16:47 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=929464)[0;0m ERROR 02-10 11:16:55 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=929464)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=929464)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=929464)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=929464)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=929464)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=929464)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=929464)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=929464)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=929464)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=929464)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=929464)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=929464)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=929464)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=929464)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929464)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=929464)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=929464)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=929349)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=929349)[0;0m     sys.exit(main())
[0;36m(APIServer pid=929349)[0;0m              ^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=929349)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=929349)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=929349)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=929349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=929349)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=929349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=929349)[0;0m     return await main
[0;36m(APIServer pid=929349)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=929349)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=929349)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=929349)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=929349)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=929349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=929349)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=929349)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=929349)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=929349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=929349)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=929349)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=929349)[0;0m     return cls(
[0;36m(APIServer pid=929349)[0;0m            ^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=929349)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=929349)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=929349)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=929349)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=929349)[0;0m     super().__init__(
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=929349)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=929349)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=929349)[0;0m     next(self.gen)
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=929349)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=929349)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=929349)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=929349)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [utils.py:325] 
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [utils.py:325] 
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'gpu_memory_utilization': 0.6, 'max_num_batched_tokens': 8192}
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:50 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:51 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=929749)[0;0m INFO 02-10 11:17:51 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=929749)[0;0m WARNING 02-10 11:17:51 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=929749)[0;0m WARNING 02-10 11:17:51 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=929864)[0;0m INFO 02-10 11:18:02 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=929864)[0;0m WARNING 02-10 11:18:02 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=929864)[0;0m INFO 02-10 11:18:04 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:39669 backend=nccl
[0;36m(EngineCore_DP0 pid=929864)[0;0m INFO 02-10 11:18:04 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=929864)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=929864)[0;0m INFO 02-10 11:18:13 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(EngineCore_DP0 pid=929864)[0;0m INFO 02-10 11:18:14 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=929864)[0;0m INFO 02-10 11:18:14 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=929864)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=929864)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=929864)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=929864)[0;0m INFO 02-10 11:18:17 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [gpu_model_runner.py:4128] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     raise e
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     + [
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]       ^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     data=torch.empty(
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m ERROR 02-10 11:18:19 [core.py:946] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=929864)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=929864)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=929864)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=929864)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=929864)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=929864)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=929864)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=929864)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=929864)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=929864)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=929864)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=929864)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=929864)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=929864)[0;0m                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=929864)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=929864)[0;0m     + [
[0;36m(EngineCore_DP0 pid=929864)[0;0m       ^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=929864)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=929864)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=929864)[0;0m     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=929864)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=929864)[0;0m                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=929864)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=929864)[0;0m     data=torch.empty(
[0;36m(EngineCore_DP0 pid=929864)[0;0m          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=929864)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=929864)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=929864)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W210 11:18:19.369898000 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=929749)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=929749)[0;0m     sys.exit(main())
[0;36m(APIServer pid=929749)[0;0m              ^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=929749)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=929749)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=929749)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=929749)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=929749)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=929749)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=929749)[0;0m     return await main
[0;36m(APIServer pid=929749)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=929749)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=929749)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=929749)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=929749)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=929749)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=929749)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=929749)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=929749)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=929749)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=929749)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=929749)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=929749)[0;0m     return cls(
[0;36m(APIServer pid=929749)[0;0m            ^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=929749)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=929749)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=929749)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=929749)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=929749)[0;0m     super().__init__(
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=929749)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=929749)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=929749)[0;0m     next(self.gen)
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=929749)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=929749)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=929749)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=929749)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [utils.py:325] 
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [utils.py:325] 
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'gpu_memory_utilization': 0.6, 'max_num_batched_tokens': 8192, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=930244)[0;0m INFO 02-10 11:19:32 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=930244)[0;0m WARNING 02-10 11:19:32 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=930244)[0;0m WARNING 02-10 11:19:32 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=930376)[0;0m INFO 02-10 11:19:43 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=930376)[0;0m WARNING 02-10 11:19:43 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=930376)[0;0m INFO 02-10 11:19:45 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:59749 backend=nccl
[0;36m(EngineCore_DP0 pid=930376)[0;0m INFO 02-10 11:19:45 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=930376)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=930376)[0;0m INFO 02-10 11:19:53 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(EngineCore_DP0 pid=930376)[0;0m INFO 02-10 11:19:54 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=930376)[0;0m INFO 02-10 11:19:54 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=930376)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=930376)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=930376)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=930376)[0;0m INFO 02-10 11:19:57 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [gpu_model_runner.py:4128] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     raise e
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     + [
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]       ^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     data=torch.empty(
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m ERROR 02-10 11:19:59 [core.py:946] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=930376)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=930376)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=930376)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=930376)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=930376)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=930376)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=930376)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=930376)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930376)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=930376)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930376)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=930376)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930376)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=930376)[0;0m                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=930376)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=930376)[0;0m     + [
[0;36m(EngineCore_DP0 pid=930376)[0;0m       ^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=930376)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=930376)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=930376)[0;0m     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=930376)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=930376)[0;0m                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=930376)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=930376)[0;0m     data=torch.empty(
[0;36m(EngineCore_DP0 pid=930376)[0;0m          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=930376)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=930376)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930376)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W210 11:19:59.573598272 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=930244)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=930244)[0;0m     sys.exit(main())
[0;36m(APIServer pid=930244)[0;0m              ^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=930244)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=930244)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=930244)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=930244)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=930244)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=930244)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=930244)[0;0m     return await main
[0;36m(APIServer pid=930244)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=930244)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=930244)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=930244)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=930244)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=930244)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=930244)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=930244)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=930244)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=930244)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=930244)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=930244)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=930244)[0;0m     return cls(
[0;36m(APIServer pid=930244)[0;0m            ^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=930244)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=930244)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=930244)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=930244)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=930244)[0;0m     super().__init__(
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=930244)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=930244)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=930244)[0;0m     next(self.gen)
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=930244)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=930244)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=930244)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=930244)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [utils.py:325] 
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [utils.py:325] 
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'max_num_batched_tokens': 8192, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=930724)[0;0m INFO 02-10 11:21:05 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=930724)[0;0m WARNING 02-10 11:21:05 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=930724)[0;0m WARNING 02-10 11:21:05 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=930839)[0;0m INFO 02-10 11:21:16 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=930839)[0;0m WARNING 02-10 11:21:16 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=930839)[0;0m INFO 02-10 11:21:18 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:45063 backend=nccl
[0;36m(EngineCore_DP0 pid=930839)[0;0m INFO 02-10 11:21:18 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=930839)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=930839)[0;0m INFO 02-10 11:21:27 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(EngineCore_DP0 pid=930839)[0;0m INFO 02-10 11:21:28 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=930839)[0;0m INFO 02-10 11:21:28 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=930839)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=930839)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=930839)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=930839)[0;0m INFO 02-10 11:21:31 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [gpu_model_runner.py:4128] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     raise e
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     + [
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]       ^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     data=torch.empty(
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m ERROR 02-10 11:21:32 [core.py:946] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=930839)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=930839)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=930839)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=930839)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=930839)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=930839)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=930839)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=930839)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930839)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=930839)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930839)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=930839)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=930839)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=930839)[0;0m                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=930839)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=930839)[0;0m     + [
[0;36m(EngineCore_DP0 pid=930839)[0;0m       ^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=930839)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=930839)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=930839)[0;0m     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=930839)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=930839)[0;0m                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=930839)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=930839)[0;0m     data=torch.empty(
[0;36m(EngineCore_DP0 pid=930839)[0;0m          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=930839)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=930839)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=930839)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W210 11:21:33.734839021 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=930724)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=930724)[0;0m     sys.exit(main())
[0;36m(APIServer pid=930724)[0;0m              ^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=930724)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=930724)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=930724)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=930724)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=930724)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=930724)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=930724)[0;0m     return await main
[0;36m(APIServer pid=930724)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=930724)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=930724)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=930724)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=930724)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=930724)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=930724)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=930724)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=930724)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=930724)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=930724)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=930724)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=930724)[0;0m     return cls(
[0;36m(APIServer pid=930724)[0;0m            ^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=930724)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=930724)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=930724)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=930724)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=930724)[0;0m     super().__init__(
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=930724)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=930724)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=930724)[0;0m     next(self.gen)
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=930724)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=930724)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=930724)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=930724)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [utils.py:325] 
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [utils.py:325] 
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'max_num_batched_tokens': 8192, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:30 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:31 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=931265)[0;0m INFO 02-10 12:43:31 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=931265)[0;0m WARNING 02-10 12:43:31 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=931265)[0;0m WARNING 02-10 12:43:31 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=931404)[0;0m INFO 02-10 12:43:41 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=931404)[0;0m WARNING 02-10 12:43:42 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=931404)[0;0m INFO 02-10 12:43:44 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:56571 backend=nccl
[0;36m(EngineCore_DP0 pid=931404)[0;0m INFO 02-10 12:43:44 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=931404)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=931404)[0;0m INFO 02-10 12:43:53 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(EngineCore_DP0 pid=931404)[0;0m INFO 02-10 12:43:53 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=931404)[0;0m INFO 02-10 12:43:54 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=931404)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=931404)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=931404)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=931404)[0;0m INFO 02-10 12:43:58 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [gpu_model_runner.py:4128] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     raise e
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     + [
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]       ^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     data=torch.empty(
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m ERROR 02-10 12:43:59 [core.py:946] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=931404)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=931404)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=931404)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=931404)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=931404)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=931404)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=931404)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=931404)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931404)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=931404)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931404)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=931404)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931404)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=931404)[0;0m                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=931404)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=931404)[0;0m     + [
[0;36m(EngineCore_DP0 pid=931404)[0;0m       ^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=931404)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=931404)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=931404)[0;0m     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=931404)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=931404)[0;0m                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=931404)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=931404)[0;0m     data=torch.empty(
[0;36m(EngineCore_DP0 pid=931404)[0;0m          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=931404)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=931404)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931404)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W210 12:44:00.466331580 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=931265)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=931265)[0;0m     sys.exit(main())
[0;36m(APIServer pid=931265)[0;0m              ^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=931265)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=931265)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=931265)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=931265)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=931265)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=931265)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=931265)[0;0m     return await main
[0;36m(APIServer pid=931265)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=931265)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=931265)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=931265)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=931265)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=931265)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=931265)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=931265)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=931265)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=931265)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=931265)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=931265)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=931265)[0;0m     return cls(
[0;36m(APIServer pid=931265)[0;0m            ^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=931265)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=931265)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=931265)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=931265)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=931265)[0;0m     super().__init__(
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=931265)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=931265)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=931265)[0;0m     next(self.gen)
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=931265)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=931265)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=931265)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=931265)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [utils.py:325] 
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [utils.py:325] 
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'max_num_batched_tokens': 8192, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=931764)[0;0m INFO 02-10 12:44:35 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=931764)[0;0m WARNING 02-10 12:44:35 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=931764)[0;0m WARNING 02-10 12:44:35 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=931900)[0;0m INFO 02-10 12:44:46 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=931900)[0;0m WARNING 02-10 12:44:46 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=931900)[0;0m INFO 02-10 12:44:48 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:34379 backend=nccl
[0;36m(EngineCore_DP0 pid=931900)[0;0m INFO 02-10 12:44:48 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=931900)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=931900)[0;0m INFO 02-10 12:44:58 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(EngineCore_DP0 pid=931900)[0;0m INFO 02-10 12:44:58 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=931900)[0;0m INFO 02-10 12:44:58 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=931900)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=931900)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=931900)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=931900)[0;0m INFO 02-10 12:45:01 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [gpu_model_runner.py:4128] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     raise e
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     + [
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]       ^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     data=torch.empty(
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m ERROR 02-10 12:45:03 [core.py:946] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=931900)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=931900)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=931900)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=931900)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=931900)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=931900)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=931900)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=931900)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931900)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 534, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=931900)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 366, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931900)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=931900)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=931900)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 472, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.model = Gemma3Model(
[0;36m(EngineCore_DP0 pid=931900)[0;0m                  ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 318, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=931900)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=931900)[0;0m     + [
[0;36m(EngineCore_DP0 pid=931900)[0;0m       ^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=931900)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=931900)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 320, in <lambda>
[0;36m(EngineCore_DP0 pid=931900)[0;0m     lambda prefix: Gemma3DecoderLayer(
[0;36m(EngineCore_DP0 pid=931900)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 257, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.mlp = Gemma3MLP(
[0;36m(EngineCore_DP0 pid=931900)[0;0m                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py", line 77, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=931900)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 670, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 495, in __init__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 224, in create_weights
[0;36m(EngineCore_DP0 pid=931900)[0;0m     data=torch.empty(
[0;36m(EngineCore_DP0 pid=931900)[0;0m          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=931900)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=931900)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=931900)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 190.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W210 12:45:04.860523896 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=931764)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=931764)[0;0m     sys.exit(main())
[0;36m(APIServer pid=931764)[0;0m              ^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=931764)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=931764)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=931764)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=931764)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=931764)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=931764)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=931764)[0;0m     return await main
[0;36m(APIServer pid=931764)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=931764)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=931764)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=931764)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=931764)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=931764)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=931764)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=931764)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=931764)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=931764)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=931764)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=931764)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=931764)[0;0m     return cls(
[0;36m(APIServer pid=931764)[0;0m            ^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=931764)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=931764)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=931764)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=931764)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=931764)[0;0m     super().__init__(
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=931764)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=931764)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=931764)[0;0m     next(self.gen)
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=931764)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=931764)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=931764)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=931764)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [utils.py:325] 
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [utils.py:325] 
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 8192, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=932223)[0;0m INFO 02-10 12:45:13 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=932223)[0;0m WARNING 02-10 12:45:13 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=932223)[0;0m WARNING 02-10 12:45:13 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=932359)[0;0m INFO 02-10 12:45:24 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=932359)[0;0m WARNING 02-10 12:45:24 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 12:45:30 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 12:45:31 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 12:45:33 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:33111 backend=nccl
INFO 02-10 12:45:33 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:33111 backend=nccl
INFO 02-10 12:45:34 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 12:45:34 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 12:45:34 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 12:45:34 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 12:45:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 12:45:34 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 12:45:34 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 12:45:34 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 12:45:34 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=932359)[0;0m ERROR 02-10 12:45:42 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=932359)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=932359)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=932359)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=932359)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=932359)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=932359)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=932359)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=932359)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=932359)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=932359)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=932359)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=932359)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=932359)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=932359)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932359)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=932359)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=932359)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=932223)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=932223)[0;0m     sys.exit(main())
[0;36m(APIServer pid=932223)[0;0m              ^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=932223)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=932223)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=932223)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=932223)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=932223)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=932223)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=932223)[0;0m     return await main
[0;36m(APIServer pid=932223)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=932223)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=932223)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=932223)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=932223)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=932223)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=932223)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=932223)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=932223)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=932223)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=932223)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=932223)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=932223)[0;0m     return cls(
[0;36m(APIServer pid=932223)[0;0m            ^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=932223)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=932223)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=932223)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=932223)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=932223)[0;0m     super().__init__(
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=932223)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=932223)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=932223)[0;0m     next(self.gen)
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=932223)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=932223)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=932223)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=932223)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [utils.py:325] 
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [utils.py:325] 
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 8192, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=932759)[0;0m INFO 02-10 12:47:03 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=932759)[0;0m WARNING 02-10 12:47:03 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=932759)[0;0m WARNING 02-10 12:47:03 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=932895)[0;0m INFO 02-10 12:47:13 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=932895)[0;0m WARNING 02-10 12:47:13 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 12:47:20 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 12:47:20 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 12:47:22 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:42781 backend=nccl
INFO 02-10 12:47:22 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:42781 backend=nccl
INFO 02-10 12:47:22 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 12:47:22 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 12:47:22 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 12:47:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 12:47:22 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 12:47:22 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 12:47:22 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 12:47:22 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 12:47:22 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=932895)[0;0m ERROR 02-10 12:47:31 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=932895)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=932895)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=932895)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=932895)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=932895)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=932895)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=932895)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=932895)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=932895)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=932895)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=932895)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=932895)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=932895)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=932895)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=932895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=932895)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=932895)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=932759)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=932759)[0;0m     sys.exit(main())
[0;36m(APIServer pid=932759)[0;0m              ^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=932759)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=932759)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=932759)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=932759)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=932759)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=932759)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=932759)[0;0m     return await main
[0;36m(APIServer pid=932759)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=932759)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=932759)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=932759)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=932759)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=932759)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=932759)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=932759)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=932759)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=932759)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=932759)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=932759)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=932759)[0;0m     return cls(
[0;36m(APIServer pid=932759)[0;0m            ^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=932759)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=932759)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=932759)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=932759)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=932759)[0;0m     super().__init__(
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=932759)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=932759)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=932759)[0;0m     next(self.gen)
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=932759)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=932759)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=932759)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=932759)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [utils.py:325] 
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [utils.py:325] 
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 8192, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=933178)[0;0m INFO 02-10 12:47:46 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=933178)[0;0m WARNING 02-10 12:47:46 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=933178)[0;0m WARNING 02-10 12:47:46 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=933314)[0;0m INFO 02-10 12:47:56 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=933314)[0;0m WARNING 02-10 12:47:56 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 12:48:04 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 12:48:04 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 12:48:06 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50953 backend=nccl
INFO 02-10 12:48:06 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50953 backend=nccl
INFO 02-10 12:48:07 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 12:48:07 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 12:48:07 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 12:48:07 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 12:48:07 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 12:48:07 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 12:48:07 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 12:48:07 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 12:48:07 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=933314)[0;0m ERROR 02-10 12:48:15 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=933314)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=933314)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=933314)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=933314)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=933314)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=933314)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=933314)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=933314)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=933314)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=933314)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=933314)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=933314)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=933314)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=933314)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933314)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=933314)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=933314)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=933178)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=933178)[0;0m     sys.exit(main())
[0;36m(APIServer pid=933178)[0;0m              ^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=933178)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=933178)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=933178)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=933178)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=933178)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=933178)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=933178)[0;0m     return await main
[0;36m(APIServer pid=933178)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=933178)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=933178)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=933178)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=933178)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=933178)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=933178)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=933178)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=933178)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=933178)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=933178)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=933178)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=933178)[0;0m     return cls(
[0;36m(APIServer pid=933178)[0;0m            ^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=933178)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=933178)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=933178)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=933178)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=933178)[0;0m     super().__init__(
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=933178)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=933178)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=933178)[0;0m     next(self.gen)
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=933178)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=933178)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=933178)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=933178)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [utils.py:325] 
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [utils.py:325] 
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 4096, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
[0;36m(APIServer pid=933596)[0;0m INFO 02-10 12:48:55 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=933596)[0;0m WARNING 02-10 12:48:55 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=933596)[0;0m WARNING 02-10 12:48:55 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=933711)[0;0m INFO 02-10 12:49:05 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=933711)[0;0m WARNING 02-10 12:49:05 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 12:49:13 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 12:49:13 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 12:49:15 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:43801 backend=nccl
INFO 02-10 12:49:15 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:43801 backend=nccl
INFO 02-10 12:49:15 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 12:49:16 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 12:49:16 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 12:49:16 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 12:49:16 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 12:49:16 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 12:49:16 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 12:49:16 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 12:49:16 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=933711)[0;0m ERROR 02-10 12:49:24 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=933711)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=933711)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=933711)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=933711)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=933711)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=933711)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=933711)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=933711)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=933711)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=933711)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=933711)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=933711)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=933711)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=933711)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=933711)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=933711)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=933711)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=933596)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=933596)[0;0m     sys.exit(main())
[0;36m(APIServer pid=933596)[0;0m              ^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=933596)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=933596)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=933596)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=933596)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=933596)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=933596)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=933596)[0;0m     return await main
[0;36m(APIServer pid=933596)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=933596)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=933596)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=933596)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=933596)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=933596)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=933596)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=933596)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=933596)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=933596)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=933596)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=933596)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=933596)[0;0m     return cls(
[0;36m(APIServer pid=933596)[0;0m            ^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=933596)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=933596)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=933596)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=933596)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=933596)[0;0m     super().__init__(
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=933596)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=933596)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=933596)[0;0m     next(self.gen)
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=933596)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=933596)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=933596)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=933596)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [utils.py:325] 
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [utils.py:325] 
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 2048, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=933989)[0;0m INFO 02-10 12:51:16 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=933989)[0;0m WARNING 02-10 12:51:16 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=933989)[0;0m WARNING 02-10 12:51:16 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=934104)[0;0m INFO 02-10 12:51:27 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=934104)[0;0m WARNING 02-10 12:51:27 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 12:51:34 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 12:51:34 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 12:51:36 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:36089 backend=nccl
INFO 02-10 12:51:36 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:36089 backend=nccl
INFO 02-10 12:51:36 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 12:51:37 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 12:51:37 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 12:51:37 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 12:51:37 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 12:51:37 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 12:51:37 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 12:51:37 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 12:51:37 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=934104)[0;0m ERROR 02-10 12:51:45 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=934104)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=934104)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=934104)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=934104)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=934104)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=934104)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=934104)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=934104)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=934104)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=934104)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=934104)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=934104)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=934104)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=934104)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934104)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=934104)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=934104)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=933989)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=933989)[0;0m     sys.exit(main())
[0;36m(APIServer pid=933989)[0;0m              ^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=933989)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=933989)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=933989)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=933989)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=933989)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=933989)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=933989)[0;0m     return await main
[0;36m(APIServer pid=933989)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=933989)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=933989)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=933989)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=933989)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=933989)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=933989)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=933989)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=933989)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=933989)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=933989)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=933989)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=933989)[0;0m     return cls(
[0;36m(APIServer pid=933989)[0;0m            ^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=933989)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=933989)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=933989)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=933989)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=933989)[0;0m     super().__init__(
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=933989)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=933989)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=933989)[0;0m     next(self.gen)
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=933989)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=933989)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=933989)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=933989)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [utils.py:325] 
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [utils.py:325] 
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 2048, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:52 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:53 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=934421)[0;0m INFO 02-10 12:57:53 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=934421)[0;0m WARNING 02-10 12:57:53 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=934421)[0;0m WARNING 02-10 12:57:53 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=934536)[0;0m INFO 02-10 12:58:03 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=934536)[0;0m WARNING 02-10 12:58:03 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 12:58:11 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 12:58:11 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 12:58:13 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:51611 backend=nccl
INFO 02-10 12:58:13 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:51611 backend=nccl
INFO 02-10 12:58:14 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 12:58:14 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 12:58:14 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 12:58:14 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 12:58:14 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 12:58:14 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 12:58:14 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 12:58:14 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 12:58:14 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=934536)[0;0m ERROR 02-10 12:58:22 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=934536)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=934536)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=934536)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=934536)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=934536)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=934536)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=934536)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=934536)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=934536)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=934536)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=934536)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=934536)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=934536)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=934536)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=934536)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=934536)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=934536)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=934421)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=934421)[0;0m     sys.exit(main())
[0;36m(APIServer pid=934421)[0;0m              ^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=934421)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=934421)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=934421)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=934421)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=934421)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=934421)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=934421)[0;0m     return await main
[0;36m(APIServer pid=934421)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=934421)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=934421)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=934421)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=934421)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=934421)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=934421)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=934421)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=934421)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=934421)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=934421)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=934421)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=934421)[0;0m     return cls(
[0;36m(APIServer pid=934421)[0;0m            ^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=934421)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=934421)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=934421)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=934421)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=934421)[0;0m     super().__init__(
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=934421)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=934421)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=934421)[0;0m     next(self.gen)
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=934421)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=934421)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=934421)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=934421)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [utils.py:325] 
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [utils.py:325] 
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 2048, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=935043)[0;0m INFO 02-10 13:03:58 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=935043)[0;0m WARNING 02-10 13:03:58 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=935043)[0;0m WARNING 02-10 13:03:58 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=935182)[0;0m INFO 02-10 13:04:09 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=935182)[0;0m WARNING 02-10 13:04:09 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 13:04:17 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 13:04:17 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 13:04:19 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:41001 backend=nccl
INFO 02-10 13:04:19 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:41001 backend=nccl
INFO 02-10 13:04:20 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 13:04:20 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 13:04:20 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 13:04:20 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 13:04:20 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 13:04:20 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 13:04:20 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
INFO 02-10 13:04:20 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 13:04:20 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=935182)[0;0m ERROR 02-10 13:04:28 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=935182)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=935182)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=935182)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=935182)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=935182)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=935182)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=935182)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=935182)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=935182)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=935182)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=935182)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=935182)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=935182)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=935182)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935182)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=935182)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=935182)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=935043)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=935043)[0;0m     sys.exit(main())
[0;36m(APIServer pid=935043)[0;0m              ^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=935043)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=935043)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=935043)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=935043)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=935043)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=935043)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=935043)[0;0m     return await main
[0;36m(APIServer pid=935043)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=935043)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=935043)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=935043)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=935043)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=935043)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=935043)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=935043)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=935043)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=935043)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=935043)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=935043)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=935043)[0;0m     return cls(
[0;36m(APIServer pid=935043)[0;0m            ^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=935043)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=935043)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=935043)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=935043)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=935043)[0;0m     super().__init__(
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=935043)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=935043)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=935043)[0;0m     next(self.gen)
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=935043)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=935043)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=935043)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=935043)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [utils.py:325] 
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [utils.py:325] 
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 2048, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=935488)[0;0m INFO 02-10 13:05:27 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=935488)[0;0m WARNING 02-10 13:05:27 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=935488)[0;0m WARNING 02-10 13:05:28 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:05:37 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=935627)[0;0m WARNING 02-10 13:05:37 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:05:39 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:34331 backend=nccl
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:05:39 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=935627)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:05:48 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:05:49 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:05:49 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=935627)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=935627)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=935627)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:05:53 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=935627)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=935627)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:08<00:35,  8.96s/it]
[0;36m(EngineCore_DP0 pid=935627)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:16<00:23,  8.00s/it]
[0;36m(EngineCore_DP0 pid=935627)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:24<00:16,  8.02s/it]
[0;36m(EngineCore_DP0 pid=935627)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:31<00:07,  7.77s/it]
[0;36m(EngineCore_DP0 pid=935627)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:40<00:00,  8.14s/it]
[0;36m(EngineCore_DP0 pid=935627)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:40<00:00,  8.11s/it]
[0;36m(EngineCore_DP0 pid=935627)[0;0m 
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:06:35 [default_loader.py:291] Loading weights took 38.68 seconds
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:06:35 [gpu_model_runner.py:4130] Model loading took 23.31 GiB memory and 43.937002 seconds
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:06:36 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:07:11 [backends.py:812] Using cache directory: /home/h202403659/.cache/vllm/torch_compile_cache/ffd06ac99a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=935627)[0;0m INFO 02-10 13:07:11 [backends.py:872] Dynamo bytecode transform time: 13.55 s
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 322, in determine_available_memory
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4986, in profile_run
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4697, in _dummy_run
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 630, in forward
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     hidden_states = self.language_model.model(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 561, in __call__
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1211, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     torch._dynamo.repro.after_aot.save_graph_repro(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py", line 497, in save_graph_repro
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     generate_compiler_repro_string(
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py", line 362, in generate_compiler_repro_string
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     model_str += _cuda_system_info_comment()
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_dynamo/debug_utils.py", line 265, in _cuda_system_info_comment
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     cuda_version_out = subprocess.check_output(["nvcc", "--version"])
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/usr/lib/python3.11/subprocess.py", line 465, in check_output
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/usr/lib/python3.11/subprocess.py", line 546, in run
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     with Popen(*popenargs, **kwargs) as process:
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/usr/lib/python3.11/subprocess.py", line 1022, in __init__
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     self._execute_child(args, executable, preexec_fn, close_fds,
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]   File "/usr/lib/python3.11/subprocess.py", line 1899, in _execute_child
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946]     raise child_exception_type(errno_num, err_msg, err_filename)
[0;36m(EngineCore_DP0 pid=935627)[0;0m ERROR 02-10 13:07:25 [core.py:946] torch._inductor.exc.InductorError: PermissionError: [Errno 13] Permission denied: 'nvcc'
[0;36m(EngineCore_DP0 pid=935627)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=935627)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=935627)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=935627)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=935627)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=935627)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=935627)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=935627)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=935627)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=935627)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=935627)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=935627)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 322, in determine_available_memory
[0;36m(EngineCore_DP0 pid=935627)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4986, in profile_run
[0;36m(EngineCore_DP0 pid=935627)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=935627)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4697, in _dummy_run
[0;36m(EngineCore_DP0 pid=935627)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=935627)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/model_executor/models/gemma3_mm.py", line 630, in forward
[0;36m(EngineCore_DP0 pid=935627)[0;0m     hidden_states = self.language_model.model(
[0;36m(EngineCore_DP0 pid=935627)[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/decorators.py", line 561, in __call__
[0;36m(EngineCore_DP0 pid=935627)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[0;36m(EngineCore_DP0 pid=935627)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=935627)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=935627)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=935627)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=935627)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=935627)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1211, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=935627)[0;0m     torch._dynamo.repro.after_aot.save_graph_repro(
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py", line 497, in save_graph_repro
[0;36m(EngineCore_DP0 pid=935627)[0;0m     generate_compiler_repro_string(
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py", line 362, in generate_compiler_repro_string
[0;36m(EngineCore_DP0 pid=935627)[0;0m     model_str += _cuda_system_info_comment()
[0;36m(EngineCore_DP0 pid=935627)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/torch/_dynamo/debug_utils.py", line 265, in _cuda_system_info_comment
[0;36m(EngineCore_DP0 pid=935627)[0;0m     cuda_version_out = subprocess.check_output(["nvcc", "--version"])
[0;36m(EngineCore_DP0 pid=935627)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/usr/lib/python3.11/subprocess.py", line 465, in check_output
[0;36m(EngineCore_DP0 pid=935627)[0;0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
[0;36m(EngineCore_DP0 pid=935627)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/usr/lib/python3.11/subprocess.py", line 546, in run
[0;36m(EngineCore_DP0 pid=935627)[0;0m     with Popen(*popenargs, **kwargs) as process:
[0;36m(EngineCore_DP0 pid=935627)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/usr/lib/python3.11/subprocess.py", line 1022, in __init__
[0;36m(EngineCore_DP0 pid=935627)[0;0m     self._execute_child(args, executable, preexec_fn, close_fds,
[0;36m(EngineCore_DP0 pid=935627)[0;0m   File "/usr/lib/python3.11/subprocess.py", line 1899, in _execute_child
[0;36m(EngineCore_DP0 pid=935627)[0;0m     raise child_exception_type(errno_num, err_msg, err_filename)
[0;36m(EngineCore_DP0 pid=935627)[0;0m torch._inductor.exc.InductorError: PermissionError: [Errno 13] Permission denied: 'nvcc'
[rank0]:[W210 13:07:26.644620467 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=935488)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=935488)[0;0m     sys.exit(main())
[0;36m(APIServer pid=935488)[0;0m              ^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=935488)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=935488)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=935488)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=935488)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=935488)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=935488)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=935488)[0;0m     return await main
[0;36m(APIServer pid=935488)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=935488)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=935488)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=935488)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=935488)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=935488)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=935488)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=935488)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=935488)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=935488)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=935488)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=935488)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=935488)[0;0m     return cls(
[0;36m(APIServer pid=935488)[0;0m            ^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=935488)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=935488)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=935488)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=935488)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=935488)[0;0m     super().__init__(
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=935488)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=935488)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=935488)[0;0m     next(self.gen)
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=935488)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=935488)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=935488)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=935488)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:45 [utils.py:325] 
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:45 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:45 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:45 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:45 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:45 [utils.py:325] 
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:46 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 2048, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:46 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:46 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:46 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=936253)[0;0m INFO 02-10 13:08:46 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=936253)[0;0m WARNING 02-10 13:08:46 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=936253)[0;0m WARNING 02-10 13:08:46 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:08:55 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=936368)[0;0m WARNING 02-10 13:08:55 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:08:57 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:40695 backend=nccl
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:08:57 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=936368)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:09:06 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:09:06 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:09:06 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=936368)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=936368)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=936368)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:09:09 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=936368)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=936368)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:04,  1.15s/it]
[0;36m(EngineCore_DP0 pid=936368)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.25s/it]
[0;36m(EngineCore_DP0 pid=936368)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:03<00:02,  1.23s/it]
[0;36m(EngineCore_DP0 pid=936368)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:05<00:01,  1.54s/it]
[0;36m(EngineCore_DP0 pid=936368)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:06<00:00,  1.40s/it]
[0;36m(EngineCore_DP0 pid=936368)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:06<00:00,  1.37s/it]
[0;36m(EngineCore_DP0 pid=936368)[0;0m 
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:09:18 [default_loader.py:291] Loading weights took 6.20 seconds
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:09:19 [gpu_model_runner.py:4130] Model loading took 23.31 GiB memory and 11.309505 seconds
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:09:19 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
[0;36m(EngineCore_DP0 pid=936368)[0;0m INFO 02-10 13:12:20 [gpu_worker.py:356] Available KV cache memory: -5.76 GiB
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 253, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 1516, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 616, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946]     raise ValueError(
[0;36m(EngineCore_DP0 pid=936368)[0;0m ERROR 02-10 13:12:20 [core.py:946] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=936368)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=936368)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=936368)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=936368)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=936368)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=936368)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=936368)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=936368)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=936368)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=936368)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 253, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=936368)[0;0m     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=936368)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 1516, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=936368)[0;0m     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=936368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 616, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=936368)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=936368)[0;0m ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W210 13:12:21.523673282 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=936253)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=936253)[0;0m     sys.exit(main())
[0;36m(APIServer pid=936253)[0;0m              ^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=936253)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=936253)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=936253)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=936253)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=936253)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=936253)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=936253)[0;0m     return await main
[0;36m(APIServer pid=936253)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=936253)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=936253)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=936253)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=936253)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=936253)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=936253)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=936253)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=936253)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=936253)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=936253)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=936253)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=936253)[0;0m     return cls(
[0;36m(APIServer pid=936253)[0;0m            ^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=936253)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=936253)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=936253)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=936253)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=936253)[0;0m     super().__init__(
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=936253)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=936253)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=936253)[0;0m     next(self.gen)
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=936253)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=936253)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=936253)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=936253)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [utils.py:325] 
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [utils.py:325] 
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 4096, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.95, 'max_num_batched_tokens': 1024, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [model.py:1561] Using max model len 4096
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=1024.
[0;36m(APIServer pid=936809)[0;0m INFO 02-10 13:14:09 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=936809)[0;0m WARNING 02-10 13:14:09 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=936809)[0;0m WARNING 02-10 13:14:09 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=936924)[0;0m INFO 02-10 13:14:19 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [1024], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=936924)[0;0m WARNING 02-10 13:14:19 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 13:14:24 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 13:14:25 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 13:14:27 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:37889 backend=nccl
INFO 02-10 13:14:27 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:37889 backend=nccl
INFO 02-10 13:14:27 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-10 13:14:28 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-10 13:14:28 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-10 13:14:28 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-10 13:14:28 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 02-10 13:14:28 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-10 13:14:28 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
ERROR 02-10 13:14:28 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 13:14:28 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 13:14:28 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
ERROR 02-10 13:14:28 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     raise ValueError(
ERROR 02-10 13:14:28 [multiproc_executor.py:772] ValueError: Free memory on device cuda:1 (22.6/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
INFO 02-10 13:14:28 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 13:14:28 [multiproc_executor.py:730] Parent process exited, terminating worker
ERROR 02-10 13:14:28 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 13:14:28 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 13:14:28 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
ERROR 02-10 13:14:28 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:14:28 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
ERROR 02-10 13:14:28 [multiproc_executor.py:772]     raise ValueError(
ERROR 02-10 13:14:28 [multiproc_executor.py:772] ValueError: Free memory on device cuda:0 (22.6/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W210 13:14:28.987037305 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=936924)[0;0m ERROR 02-10 13:14:29 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=936924)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=936924)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=936924)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=936924)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=936924)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=936924)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=936924)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=936924)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=936924)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=936924)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=936924)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=936924)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=936924)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=936924)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=936924)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=936924)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=936924)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=936809)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=936809)[0;0m     sys.exit(main())
[0;36m(APIServer pid=936809)[0;0m              ^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=936809)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=936809)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=936809)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=936809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=936809)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=936809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=936809)[0;0m     return await main
[0;36m(APIServer pid=936809)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=936809)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=936809)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=936809)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=936809)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=936809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=936809)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=936809)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=936809)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=936809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=936809)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=936809)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=936809)[0;0m     return cls(
[0;36m(APIServer pid=936809)[0;0m            ^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=936809)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=936809)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=936809)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=936809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=936809)[0;0m     super().__init__(
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=936809)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=936809)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=936809)[0;0m     next(self.gen)
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=936809)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=936809)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=936809)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=936809)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
/usr/lib/python3.11/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [utils.py:325] 
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [utils.py:325] 
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 4096, 'enforce_eager': True, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.95, 'max_num_batched_tokens': 1024, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [model.py:1561] Using max model len 4096
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=1024.
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=937346)[0;0m WARNING 02-10 13:16:43 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=937346)[0;0m INFO 02-10 13:16:43 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=937346)[0;0m WARNING 02-10 13:16:43 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=937346)[0;0m WARNING 02-10 13:16:43 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=937461)[0;0m INFO 02-10 13:16:52 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [1024], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=937461)[0;0m WARNING 02-10 13:16:52 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 13:16:57 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 13:16:58 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 13:17:00 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:58089 backend=nccl
INFO 02-10 13:17:00 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:58089 backend=nccl
INFO 02-10 13:17:00 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 13:17:01 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 13:17:01 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 13:17:01 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 13:17:01 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 13:17:01 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 13:17:01 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)

[2026-02-10 13:17:01] MPG-S0097:937552:937645 [1] include/alloc.h:65 NCCL WARN Cuda failure 2 'out of memory'
INFO 02-10 13:17:01 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 13:17:01 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=937461)[0;0m ERROR 02-10 13:17:09 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=937461)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=937461)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=937461)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=937461)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=937461)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=937461)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=937461)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=937461)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=937461)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=937461)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=937461)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=937461)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=937461)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=937461)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937461)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=937461)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=937461)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=937346)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=937346)[0;0m     sys.exit(main())
[0;36m(APIServer pid=937346)[0;0m              ^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=937346)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=937346)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=937346)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=937346)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=937346)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=937346)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=937346)[0;0m     return await main
[0;36m(APIServer pid=937346)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=937346)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=937346)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=937346)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=937346)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=937346)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=937346)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=937346)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=937346)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=937346)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=937346)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=937346)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=937346)[0;0m     return cls(
[0;36m(APIServer pid=937346)[0;0m            ^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=937346)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=937346)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=937346)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=937346)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=937346)[0;0m     super().__init__(
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=937346)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=937346)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=937346)[0;0m     next(self.gen)
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=937346)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=937346)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=937346)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=937346)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [utils.py:325] 
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [utils.py:325] 
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 4096, 'enforce_eager': True, 'tensor_parallel_size': 2, 'disable_custom_all_reduce': True, 'gpu_memory_utilization': 0.95, 'max_num_batched_tokens': 1024, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [model.py:1561] Using max model len 4096
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=1024.
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=937778)[0;0m WARNING 02-10 13:19:11 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=937778)[0;0m INFO 02-10 13:19:11 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=937778)[0;0m WARNING 02-10 13:19:11 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=937778)[0;0m WARNING 02-10 13:19:11 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=937894)[0;0m INFO 02-10 13:19:20 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [1024], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=937894)[0;0m WARNING 02-10 13:19:20 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 13:19:26 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 13:19:26 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 13:19:29 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:49229 backend=nccl
INFO 02-10 13:19:29 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:49229 backend=nccl
INFO 02-10 13:19:29 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 13:19:29 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 13:19:29 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 13:19:29 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 13:19:29 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 13:19:29 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 13:19:29 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)

[2026-02-10 13:19:29] MPG-S0097:937985:938079 [1] include/alloc.h:65 NCCL WARN Cuda failure 2 'out of memory'
INFO 02-10 13:19:29 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 13:19:29 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=937894)[0;0m ERROR 02-10 13:19:37 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=937894)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=937894)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=937894)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=937894)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=937894)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=937894)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=937894)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=937894)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=937894)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=937894)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=937894)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=937894)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=937894)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=937894)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=937894)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=937894)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=937894)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=937778)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=937778)[0;0m     sys.exit(main())
[0;36m(APIServer pid=937778)[0;0m              ^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=937778)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=937778)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=937778)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=937778)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=937778)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=937778)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=937778)[0;0m     return await main
[0;36m(APIServer pid=937778)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=937778)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=937778)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=937778)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=937778)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=937778)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=937778)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=937778)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=937778)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=937778)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=937778)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=937778)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=937778)[0;0m     return cls(
[0;36m(APIServer pid=937778)[0;0m            ^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=937778)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=937778)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=937778)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=937778)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=937778)[0;0m     super().__init__(
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=937778)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=937778)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=937778)[0;0m     next(self.gen)
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=937778)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=937778)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=937778)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=937778)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [utils.py:325] 
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [utils.py:325] 
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 4096, 'enforce_eager': True, 'tensor_parallel_size': 2, 'disable_custom_all_reduce': True, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 1024, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:55 [model.py:1561] Using max model len 4096
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:56 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=1024.
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:56 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=938247)[0;0m WARNING 02-10 13:46:56 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=938247)[0;0m INFO 02-10 13:46:56 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=938247)[0;0m WARNING 02-10 13:46:56 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=938247)[0;0m WARNING 02-10 13:46:56 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=938389)[0;0m INFO 02-10 13:47:23 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [1024], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=938389)[0;0m WARNING 02-10 13:47:23 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 13:48:05 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 13:48:05 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 13:49:12 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:38973 backend=nccl
INFO 02-10 13:49:12 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:38973 backend=nccl
INFO 02-10 13:49:13 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 13:49:34 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 13:49:34 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 13:49:34 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 13:49:34 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 13:49:34 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 13:49:34 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)

[2026-02-10 13:49:34] MPG-S0097:938486:938708 [1] include/alloc.h:65 NCCL WARN Cuda failure 2 'out of memory'
INFO 02-10 13:49:34 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 13:49:34 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=938389)[0;0m ERROR 02-10 13:49:42 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=938389)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=938389)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=938389)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=938389)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=938389)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=938389)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=938389)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=938389)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=938389)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=938389)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=938389)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=938389)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=938389)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=938389)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=938389)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=938389)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=938389)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
./serve.sh: line 113: vllm: command not found
./serve.sh: line 113: vllm: command not found
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [utils.py:325] 
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [utils.py:325] 
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 4096, 'enforce_eager': True, 'tensor_parallel_size': 2, 'disable_custom_all_reduce': True, 'gpu_memory_utilization': 0.6, 'max_num_batched_tokens': 1024, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:23 [model.py:1561] Using max model len 4096
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:24 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=1024.
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:24 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=1409)[0;0m WARNING 02-10 14:07:24 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=1409)[0;0m INFO 02-10 14:07:24 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=1409)[0;0m WARNING 02-10 14:07:24 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=1409)[0;0m WARNING 02-10 14:07:24 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=1540)[0;0m INFO 02-10 14:07:33 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [1024], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=1540)[0;0m WARNING 02-10 14:07:33 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 14:07:39 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 14:07:39 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 14:07:41 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:52295 backend=nccl
INFO 02-10 14:07:41 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:52295 backend=nccl
INFO 02-10 14:07:42 [pynccl.py:111] vLLM is using nccl==2.27.5
ERROR 02-10 14:07:42 [multiproc_executor.py:772] WorkerProc failed to start.
ERROR 02-10 14:07:42 [multiproc_executor.py:772] Traceback (most recent call last):
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
ERROR 02-10 14:07:42 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 569, in __init__
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     self.worker.init_device()
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     self.worker.init_device()  # type: ignore
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 218, in init_device
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     init_worker_distributed_environment(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 956, in init_worker_distributed_environment
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     ensure_model_parallel_initialized(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1450, in ensure_model_parallel_initialized
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     initialize_model_parallel(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1347, in initialize_model_parallel
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     _TP = init_model_parallel_group(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 1067, in init_model_parallel_group
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     return GroupCoordinator(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 362, in __init__
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     self.device_communicator = device_comm_cls(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]                                ^^^^^^^^^^^^^^^^
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     self.pynccl_comm = PyNcclCommunicator(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]                        ^^^^^^^^^^^^^^^^^^^
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 146, in __init__
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     self.all_reduce(data)
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl.py", line 172, in all_reduce
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     self.nccl.ncclAllReduce(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 429, in ncclAllReduce
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     self.NCCL_CHECK(
ERROR 02-10 14:07:42 [multiproc_executor.py:772]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 373, in NCCL_CHECK
ERROR 02-10 14:07:42 [multiproc_executor.py:772]     raise RuntimeError(f"NCCL error: {error_str}")
ERROR 02-10 14:07:42 [multiproc_executor.py:772] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)

[2026-02-10 14:07:42] MPG-S0097:1628:1724 [1] include/alloc.h:65 NCCL WARN Cuda failure 2 'out of memory'
INFO 02-10 14:07:42 [multiproc_executor.py:730] Parent process exited, terminating worker
INFO 02-10 14:07:42 [multiproc_executor.py:730] Parent process exited, terminating worker
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946]     raise e from None
[0;36m(EngineCore_DP0 pid=1540)[0;0m ERROR 02-10 14:07:50 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(EngineCore_DP0 pid=1540)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=1540)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=1540)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=1540)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=1540)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=1540)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=1540)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=1540)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=1540)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=1540)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=1540)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=1540)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[0;36m(EngineCore_DP0 pid=1540)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=1540)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=1540)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
[0;36m(EngineCore_DP0 pid=1540)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=1540)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[0;36m(APIServer pid=1409)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=1409)[0;0m     sys.exit(main())
[0;36m(APIServer pid=1409)[0;0m              ^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=1409)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=1409)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=1409)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=1409)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=1409)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=1409)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=1409)[0;0m     return await main
[0;36m(APIServer pid=1409)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=1409)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=1409)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=1409)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=1409)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=1409)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=1409)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=1409)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=1409)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=1409)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=1409)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=1409)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=1409)[0;0m     return cls(
[0;36m(APIServer pid=1409)[0;0m            ^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=1409)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=1409)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=1409)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=1409)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=1409)[0;0m     super().__init__(
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=1409)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=1409)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=1409)[0;0m     next(self.gen)
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=1409)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=1409)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=1409)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=1409)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [utils.py:325] 
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [utils.py:325] 
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 4096, 'enforce_eager': True, 'tensor_parallel_size': 2, 'disable_custom_all_reduce': True, 'gpu_memory_utilization': 0.6, 'max_num_batched_tokens': 1024, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [model.py:1561] Using max model len 4096
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=1024.
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2688)[0;0m WARNING 02-10 14:11:40 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=2688)[0;0m INFO 02-10 14:11:40 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=2688)[0;0m WARNING 02-10 14:11:40 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=2688)[0;0m WARNING 02-10 14:11:40 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=2797)[0;0m INFO 02-10 14:11:50 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [1024], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=2797)[0;0m WARNING 02-10 14:11:50 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 14:11:56 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 14:11:56 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 14:11:58 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:53677 backend=nccl
INFO 02-10 14:11:58 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:53677 backend=nccl
INFO 02-10 14:11:58 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-10 14:11:59 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-10 14:11:59 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
INFO 02-10 14:11:59 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-10 14:11:59 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:09 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(Worker_TP1 pid=2885)[0;0m INFO 02-10 14:12:09 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP1 pid=2885)[0;0m INFO 02-10 14:12:09 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP1 pid=2885)[0;0m WARNING 02-10 14:12:09 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP1 pid=2885)[0;0m INFO 02-10 14:12:09 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:09 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:10 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP0 pid=2884)[0;0m WARNING 02-10 14:12:10 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:10 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=2884)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP0 pid=2884)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP0 pid=2884)[0;0m   warnings.warn(
[0;36m(Worker_TP1 pid=2885)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP1 pid=2885)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP1 pid=2885)[0;0m   warnings.warn(
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:13 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=2884)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=2884)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:05<00:23,  5.78s/it]
[0;36m(Worker_TP0 pid=2884)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:11<00:17,  5.70s/it]
[0;36m(Worker_TP0 pid=2884)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:17<00:11,  5.90s/it]
[0;36m(Worker_TP0 pid=2884)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:22<00:05,  5.64s/it]
[0;36m(Worker_TP0 pid=2884)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:27<00:00,  5.37s/it]
[0;36m(Worker_TP0 pid=2884)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:27<00:00,  5.54s/it]
[0;36m(Worker_TP0 pid=2884)[0;0m 
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:41 [default_loader.py:291] Loading weights took 27.75 seconds
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:42 [gpu_model_runner.py:4130] Model loading took 11.97 GiB memory and 31.662718 seconds
[0;36m(Worker_TP1 pid=2885)[0;0m INFO 02-10 14:12:42 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 1024 tokens, and profiled with 4 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:42 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 1024 tokens, and profiled with 4 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=2884)[0;0m INFO 02-10 14:12:45 [gpu_worker.py:356] Available KV cache memory: -0.01 GiB
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 253, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 1516, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 616, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946]     raise ValueError(
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:45 [core.py:946] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(Worker_TP1 pid=2885)[0;0m WARNING 02-10 14:12:45 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(Worker_TP0 pid=2884)[0;0m WARNING 02-10 14:12:45 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(EngineCore_DP0 pid=2797)[0;0m ERROR 02-10 14:12:47 [multiproc_executor.py:246] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
[0;36m(EngineCore_DP0 pid=2797)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=2797)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=2797)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=2797)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=2797)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=2797)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=2797)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=2797)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=2797)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=2797)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 253, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=2797)[0;0m     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=2797)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 1516, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=2797)[0;0m     _check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=2797)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py", line 616, in _check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=2797)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=2797)[0;0m ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(APIServer pid=2688)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=2688)[0;0m     sys.exit(main())
[0;36m(APIServer pid=2688)[0;0m              ^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=2688)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=2688)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=2688)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=2688)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 120, in run
[0;36m(APIServer pid=2688)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=2688)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=2688)[0;0m     return await main
[0;36m(APIServer pid=2688)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=2688)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=2688)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=2688)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=2688)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=2688)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=2688)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=2688)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 204, in __aenter__
[0;36m(APIServer pid=2688)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=2688)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=2688)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=2688)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=2688)[0;0m     return cls(
[0;36m(APIServer pid=2688)[0;0m            ^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=2688)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=2688)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=2688)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=2688)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=2688)[0;0m     super().__init__(
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=2688)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=2688)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=2688)[0;0m     next(self.gen)
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=2688)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=2688)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=2688)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=2688)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [utils.py:325] 
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [utils.py:325] 
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 4096, 'enforce_eager': True, 'tensor_parallel_size': 2, 'disable_custom_all_reduce': True, 'gpu_memory_utilization': 0.7, 'max_num_batched_tokens': 1024, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [model.py:1561] Using max model len 4096
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=1024.
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=3449)[0;0m WARNING 02-10 14:13:07 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:13:07 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=3449)[0;0m WARNING 02-10 14:13:07 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=3449)[0;0m WARNING 02-10 14:13:07 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=3559)[0;0m INFO 02-10 14:13:16 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [1024], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=3559)[0;0m WARNING 02-10 14:13:16 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 14:13:22 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 14:13:23 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 14:13:25 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59401 backend=nccl
INFO 02-10 14:13:25 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59401 backend=nccl
INFO 02-10 14:13:25 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-10 14:13:26 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-10 14:13:26 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
INFO 02-10 14:13:26 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-10 14:13:26 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:36 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:36 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP1 pid=3647)[0;0m INFO 02-10 14:13:36 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP1 pid=3647)[0;0m INFO 02-10 14:13:36 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP1 pid=3647)[0;0m WARNING 02-10 14:13:36 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP1 pid=3647)[0;0m INFO 02-10 14:13:36 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:36 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP0 pid=3646)[0;0m WARNING 02-10 14:13:36 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:36 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=3646)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP0 pid=3646)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP0 pid=3646)[0;0m   warnings.warn(
[0;36m(Worker_TP1 pid=3647)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP1 pid=3647)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP1 pid=3647)[0;0m   warnings.warn(
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:39 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=3646)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=3646)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.47it/s]
[0;36m(Worker_TP0 pid=3646)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.31it/s]
[0;36m(Worker_TP0 pid=3646)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.35it/s]
[0;36m(Worker_TP0 pid=3646)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.42it/s]
[0;36m(Worker_TP0 pid=3646)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.40it/s]
[0;36m(Worker_TP0 pid=3646)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.39it/s]
[0;36m(Worker_TP0 pid=3646)[0;0m 
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:43 [default_loader.py:291] Loading weights took 3.73 seconds
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:44 [gpu_model_runner.py:4130] Model loading took 11.97 GiB memory and 7.166513 seconds
[0;36m(Worker_TP1 pid=3647)[0;0m INFO 02-10 14:13:45 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 1024 tokens, and profiled with 4 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:45 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 1024 tokens, and profiled with 4 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=3646)[0;0m INFO 02-10 14:13:48 [gpu_worker.py:356] Available KV cache memory: 2.38 GiB
[0;36m(EngineCore_DP0 pid=3559)[0;0m INFO 02-10 14:13:48 [kv_cache_utils.py:1307] GPU KV cache size: 13,008 tokens
[0;36m(EngineCore_DP0 pid=3559)[0;0m INFO 02-10 14:13:48 [kv_cache_utils.py:1312] Maximum concurrency for 4,096 tokens per request: 5.42x
[0;36m(EngineCore_DP0 pid=3559)[0;0m INFO 02-10 14:13:48 [core.py:272] init engine (profile, create kv cache, warmup model) took 3.13 seconds
[0;36m(EngineCore_DP0 pid=3559)[0;0m WARNING 02-10 14:13:50 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=3559)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=3559)[0;0m INFO 02-10 14:14:00 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=3559)[0;0m WARNING 02-10 14:14:00 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=3559)[0;0m INFO 02-10 14:14:00 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:00 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=3449)[0;0m WARNING 02-10 14:14:00 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_k': 64, 'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:00 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=3449)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [hf.py:310] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [serving.py:212] Chat template warmup completed in 2668.4ms
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO 02-10 14:14:03 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=3449)[0;0m INFO:     Started server process [3449]
[0;36m(APIServer pid=3449)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=3449)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [utils.py:325] 
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [utils.py:325] 
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [utils.py:261] non-default args: {'model_tag': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 4096, 'enforce_eager': True, 'tensor_parallel_size': 2, 'disable_custom_all_reduce': True, 'gpu_memory_utilization': 0.7, 'max_num_batched_tokens': 1024, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [model.py:1561] Using max model len 4096
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=1024.
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=4640)[0;0m WARNING 02-10 14:39:22 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:39:22 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=4640)[0;0m WARNING 02-10 14:39:22 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=4640)[0;0m WARNING 02-10 14:39:22 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=4785)[0;0m INFO 02-10 14:39:32 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/home/h202403659/LLM-Server/models/llm/gemma3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [1024], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=4785)[0;0m WARNING 02-10 14:39:32 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-10 14:39:38 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-10 14:39:38 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-10 14:39:40 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:39295 backend=nccl
INFO 02-10 14:39:40 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:39295 backend=nccl
INFO 02-10 14:39:40 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-10 14:39:41 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-10 14:39:41 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
INFO 02-10 14:39:41 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-10 14:39:41 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:39:51 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:39:51 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:39:51 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP0 pid=4890)[0;0m WARNING 02-10 14:39:51 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:39:51 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP1 pid=4891)[0;0m INFO 02-10 14:39:51 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP1 pid=4891)[0;0m INFO 02-10 14:39:51 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP1 pid=4891)[0;0m WARNING 02-10 14:39:51 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP1 pid=4891)[0;0m INFO 02-10 14:39:51 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=4890)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP0 pid=4890)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP0 pid=4890)[0;0m   warnings.warn(
[0;36m(Worker_TP1 pid=4891)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP1 pid=4891)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP1 pid=4891)[0;0m   warnings.warn(
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:39:54 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=4890)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=4890)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.39it/s]
[0;36m(Worker_TP0 pid=4890)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.30it/s]
[0;36m(Worker_TP0 pid=4890)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.29it/s]
[0;36m(Worker_TP0 pid=4890)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:03<00:00,  1.29it/s]
[0;36m(Worker_TP0 pid=4890)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.23it/s]
[0;36m(Worker_TP0 pid=4890)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.26it/s]
[0;36m(Worker_TP0 pid=4890)[0;0m 
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:39:59 [default_loader.py:291] Loading weights took 4.03 seconds
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:39:59 [gpu_model_runner.py:4130] Model loading took 11.97 GiB memory and 7.608153 seconds
[0;36m(Worker_TP1 pid=4891)[0;0m INFO 02-10 14:40:00 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 1024 tokens, and profiled with 4 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:40:00 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 1024 tokens, and profiled with 4 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=4890)[0;0m INFO 02-10 14:40:02 [gpu_worker.py:356] Available KV cache memory: 2.38 GiB
[0;36m(EngineCore_DP0 pid=4785)[0;0m INFO 02-10 14:40:02 [kv_cache_utils.py:1307] GPU KV cache size: 13,008 tokens
[0;36m(EngineCore_DP0 pid=4785)[0;0m INFO 02-10 14:40:02 [kv_cache_utils.py:1312] Maximum concurrency for 4,096 tokens per request: 5.42x
[0;36m(EngineCore_DP0 pid=4785)[0;0m INFO 02-10 14:40:03 [core.py:272] init engine (profile, create kv cache, warmup model) took 3.25 seconds
[0;36m(EngineCore_DP0 pid=4785)[0;0m WARNING 02-10 14:40:04 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=4785)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=4785)[0;0m INFO 02-10 14:40:14 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=4785)[0;0m WARNING 02-10 14:40:14 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=4785)[0;0m INFO 02-10 14:40:14 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:14 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=4640)[0;0m WARNING 02-10 14:40:14 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_k': 64, 'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:14 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=4640)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [hf.py:310] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [serving.py:212] Chat template warmup completed in 2768.3ms
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO 02-10 14:40:17 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=4640)[0;0m INFO:     Started server process [4640]
[0;36m(APIServer pid=4640)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=4640)[0;0m INFO:     Application startup complete.
