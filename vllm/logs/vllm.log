(APIServer pid=29908) INFO 02-11 08:49:21 [utils.py:325] 
(APIServer pid=29908) INFO 02-11 08:49:21 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
(APIServer pid=29908) INFO 02-11 08:49:21 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
(APIServer pid=29908) INFO 02-11 08:49:21 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
(APIServer pid=29908) INFO 02-11 08:49:21 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
(APIServer pid=29908) INFO 02-11 08:49:21 [utils.py:325] 
(APIServer pid=29908) INFO 02-11 08:49:21 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'dtype': 'bfloat16', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gemma-3-12b-it'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.65, 'swap_space': 8.0, 'max_num_seqs': 16}
(APIServer pid=29908) INFO 02-11 08:49:21 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
(APIServer pid=29908) INFO 02-11 08:49:21 [model.py:1561] Using max model len 8192
(APIServer pid=29908) INFO 02-11 08:49:22 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=29908) INFO 02-11 08:49:22 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=29908) WARNING 02-11 08:49:22 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
(APIServer pid=29908) INFO 02-11 08:49:22 [vllm.py:762] Cudagraph is disabled under eager mode
(APIServer pid=29908) WARNING 02-11 08:49:22 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
(APIServer pid=29908) WARNING 02-11 08:49:22 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=30004) INFO 02-11 08:49:32 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gemma-3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=30004) WARNING 02-11 08:49:32 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-11 08:49:39 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-11 08:49:39 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-11 08:49:41 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:51579 backend=nccl
INFO 02-11 08:49:41 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:51579 backend=nccl
INFO 02-11 08:49:41 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-11 08:49:41 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-11 08:49:41 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-11 08:49:41 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-11 08:49:41 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 02-11 08:49:42 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-11 08:49:42 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(Worker_TP0 pid=30073) INFO 02-11 08:49:53 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
(Worker_TP1 pid=30074) INFO 02-11 08:49:53 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
(Worker_TP1 pid=30074) INFO 02-11 08:49:53 [vllm.py:624] Asynchronous scheduling is enabled.
(Worker_TP1 pid=30074) WARNING 02-11 08:49:53 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
(Worker_TP1 pid=30074) INFO 02-11 08:49:53 [vllm.py:762] Cudagraph is disabled under eager mode
(Worker_TP0 pid=30073) INFO 02-11 08:49:53 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
(Worker_TP0 pid=30073) INFO 02-11 08:49:54 [vllm.py:624] Asynchronous scheduling is enabled.
(Worker_TP0 pid=30073) WARNING 02-11 08:49:54 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
(Worker_TP0 pid=30073) INFO 02-11 08:49:54 [vllm.py:762] Cudagraph is disabled under eager mode
(Worker_TP1 pid=30074) /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
(Worker_TP1 pid=30074) We recommend installing via `pip install torch-c-dlpack-ext`
(Worker_TP1 pid=30074)   warnings.warn(
(Worker_TP0 pid=30073) /home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
(Worker_TP0 pid=30073) We recommend installing via `pip install torch-c-dlpack-ext`
(Worker_TP0 pid=30073)   warnings.warn(
(Worker_TP0 pid=30073) INFO 02-11 08:49:56 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
(Worker_TP0 pid=30073) Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
(Worker_TP0 pid=30073) Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.42it/s]
(Worker_TP0 pid=30073) Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.31it/s]
(Worker_TP0 pid=30073) Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.24it/s]
(Worker_TP0 pid=30073) Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:03<00:00,  1.25it/s]
(Worker_TP0 pid=30073) Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.24it/s]
(Worker_TP0 pid=30073) Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.26it/s]
(Worker_TP0 pid=30073) 
(Worker_TP0 pid=30073) INFO 02-11 08:50:01 [default_loader.py:291] Loading weights took 4.03 seconds
(Worker_TP0 pid=30073) INFO 02-11 08:50:02 [gpu_model_runner.py:4130] Model loading took 11.97 GiB memory and 7.657663 seconds
(Worker_TP1 pid=30074) INFO 02-11 08:50:02 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
(Worker_TP0 pid=30073) INFO 02-11 08:50:02 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
(Worker_TP0 pid=30073) INFO 02-11 08:50:06 [gpu_worker.py:356] Available KV cache memory: 3.01 GiB
(EngineCore_DP0 pid=30004) INFO 02-11 08:50:06 [kv_cache_utils.py:1307] GPU KV cache size: 16,400 tokens
(EngineCore_DP0 pid=30004) INFO 02-11 08:50:06 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 4.17x
(EngineCore_DP0 pid=30004) INFO 02-11 08:50:07 [core.py:272] init engine (profile, create kv cache, warmup model) took 4.68 seconds
(EngineCore_DP0 pid=30004) WARNING 02-11 08:50:09 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=30004) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(EngineCore_DP0 pid=30004) INFO 02-11 08:50:19 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=30004) WARNING 02-11 08:50:19 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
(EngineCore_DP0 pid=30004) INFO 02-11 08:50:19 [vllm.py:762] Cudagraph is disabled under eager mode
(APIServer pid=29908) INFO 02-11 08:50:20 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=29908) WARNING 02-11 08:50:20 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_k': 64, 'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=29908) INFO 02-11 08:50:20 [serving.py:177] Warming up chat template processing...
(APIServer pid=29908) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(APIServer pid=29908) INFO 02-11 08:50:23 [hf.py:310] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
(APIServer pid=29908) INFO 02-11 08:50:23 [serving.py:212] Chat template warmup completed in 2829.9ms
(APIServer pid=29908) INFO 02-11 08:50:23 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:38] Available routes are:
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /docs, Methods: GET, HEAD
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=29908) INFO 02-11 08:50:23 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=29908) INFO:     Started server process [29908]
(APIServer pid=29908) INFO:     Waiting for application startup.
(APIServer pid=29908) INFO:     Application startup complete.
(APIServer pid=29908) INFO:     127.0.0.1:60260 - "GET /v1/models HTTP/1.1" 200 OK
!!!!!!! Segfault encountered !!!!!!!
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in pybind11::object pybind11::detail::object_api<pybind11::handle>::operator()<(pybind11::return_value_policy)1, pybind11::detail::args_proxy, pybind11::detail::kwargs_proxy>(pybind11::detail::args_proxy&&, pybind11::detail::kwargs_proxy&&) const
  File "<unknown>", line 0, in torch::impl::dispatch::PythonKernelHolder::operator()(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)
  File "<unknown>", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const [clone .isra.0]
  File "<unknown>", line 0, in torch::jit::invokeOperatorFromPython(c10::ArrayRef<std::shared_ptr<torch::jit::Operator> >, pybind11::args const&, pybind11::kwargs const&, std::optional<c10::DispatchKey>)
  File "<unknown>", line 0, in torch::jit::_get_operation_for_overload_or_packet(c10::ArrayRef<std::shared_ptr<torch::jit::Operator> >, c10::Symbol, pybind11::args const&, pybind11::kwargs const&, bool, std::optional<c10::DispatchKey>)
  File "<unknown>", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, c10::Symbol, pybind11::args const&, pybind11::kwargs const&, bool, std::optional<c10::DispatchKey>)
  File "<unknown>", line 0, in pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#2}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const::{lambda(pybind11::args const&, pybind11::kwargs const&)#1}, pybind11::object, pybind11::args const&, pybind11::kwargs const&, pybind11::name, pybind11::doc>(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#2}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const::{lambda(pybind11::args const&, pybind11::kwargs const&)#1}&&, pybind11::object (*)(pybind11::args const&, pybind11::kwargs const&), pybind11::name const&, pybind11::doc const&)::{lambda(pybind11::detail::function_call&)#1}::_FUN(pybind11::detail::function_call&)
  File "<unknown>", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in _PyObject_MakeTpCall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in _PyObject_MakeTpCall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyEval_EvalCode
  File "<unknown>", line 0, in PyRun_StringFlags
  File "<unknown>", line 0, in PyRun_SimpleStringFlags
  File "<unknown>", line 0, in Py_RunMain
  File "<unknown>", line 0, in Py_BytesMain
  File "<unknown>", line 0, in _start
  File "<unknown>", line 0, in 0xffffffffffffffff

!!!!!!! Segfault encountered !!!!!!!
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in pybind11::object pybind11::detail::object_api<pybind11::handle>::operator()<(pybind11::return_value_policy)1, pybind11::detail::args_proxy, pybind11::detail::kwargs_proxy>(pybind11::detail::args_proxy&&, pybind11::detail::kwargs_proxy&&) const
  File "<unknown>", line 0, in torch::impl::dispatch::PythonKernelHolder::operator()(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)
  File "<unknown>", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const [clone .isra.0]
  File "<unknown>", line 0, in torch::jit::invokeOperatorFromPython(c10::ArrayRef<std::shared_ptr<torch::jit::Operator> >, pybind11::args const&, pybind11::kwargs const&, std::optional<c10::DispatchKey>)
  File "<unknown>", line 0, in torch::jit::_get_operation_for_overload_or_packet(c10::ArrayRef<std::shared_ptr<torch::jit::Operator> >, c10::Symbol, pybind11::args const&, pybind11::kwargs const&, bool, std::optional<c10::DispatchKey>)
  File "<unknown>", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, c10::Symbol, pybind11::args const&, pybind11::kwargs const&, bool, std::optional<c10::DispatchKey>)
  File "<unknown>", line 0, in pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#2}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const::{lambda(pybind11::args const&, pybind11::kwargs const&)#1}, pybind11::object, pybind11::args const&, pybind11::kwargs const&, pybind11::name, pybind11::doc>(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#2}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const::{lambda(pybind11::args const&, pybind11::kwargs const&)#1}&&, pybind11::object (*)(pybind11::args const&, pybind11::kwargs const&), pybind11::name const&, pybind11::doc const&)::{lambda(pybind11::detail::function_call&)#1}::_FUN(pybind11::detail::function_call&)
  File "<unknown>", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in _PyObject_MakeTpCall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in _PyObject_MakeTpCall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyObject_FastCallDictTstate
  File "<unknown>", line 0, in _PyObject_Call_Prepend
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in _PyFunction_Vectorcall
  File "<unknown>", line 0, in PyObject_Call
  File "<unknown>", line 0, in _PyEval_EvalFrameDefault
  File "<unknown>", line 0, in PyEval_EvalCode
  File "<unknown>", line 0, in PyRun_StringFlags
  File "<unknown>", line 0, in PyRun_SimpleStringFlags
  File "<unknown>", line 0, in Py_RunMain
  File "<unknown>", line 0, in Py_BytesMain
  File "<unknown>", line 0, in _start
  File "<unknown>", line 0, in 0xffffffffffffffff

(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:48 [multiproc_executor.py:246] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gemma-3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}, 
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [dump_input.py:79] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=chatcmpl-848c6c98a23ae15c-87154561,prompt_token_ids_len=19,prefill_token_ids_len=None,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=64, min_p=0.0, seed=None, stop=[], stop_token_ids=[106], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]),num_computed_tokens=0,lora_request=None,prompt_embeds_shape=None)], scheduled_cached_reqs=CachedRequestData(req_ids=[],resumed_req_ids=set(),new_token_ids_lens=[],all_token_ids_lens={},new_block_ids=[],num_computed_tokens=[],num_output_tokens=[]), num_scheduled_tokens={chatcmpl-848c6c98a23ae15c-87154561: 19}, total_num_scheduled_tokens=19, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0, 0, 0, 0, 0, 2], finished_req_ids=[], free_encoder_mm_hashes=[], preempted_req_ids=[], has_structured_output_requests=false, pending_structured_output_tokens=false, num_invalid_spec_tokens=null, kv_connector_metadata=null, ec_connector_metadata=null)
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [dump_input.py:81] Dumping scheduler stats: SchedulerStats(num_running_reqs=1, num_waiting_reqs=0, step_counter=0, current_wave=0, kv_cache_usage=0.0019499512512187733, prefix_cache_stats=PrefixCacheStats(reset=False, requests=1, queries=19, hits=0, preempted_requests=0, preempted_queries=0, preempted_hits=0), connector_prefix_cache_stats=None, kv_cache_eviction_events=[], spec_decoding_stats=None, kv_connector_stats=None, waiting_lora_adapters={}, running_lora_adapters={}, cudagraph_stats=None, perf_stats=None)
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948] EngineCore encountered a fatal error.
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948] Traceback (most recent call last):
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     engine_core.run_busy_loop()
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 966, in run_busy_loop
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     self._process_engine_step()
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 999, in _process_engine_step
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     outputs, model_executed = self.step_fn()
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]                               ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 486, in step_with_batch_queue
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     model_output = future.result()
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 80, in result
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     return super().result()
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/usr/lib/python3.11/concurrent/futures/_base.py", line 449, in result
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     return self.__get_result()
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/usr/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     raise self._exception
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 84, in wait_for_response
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     response = self.aggregate(get_response())
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]                               ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 351, in get_response
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     status, result = mq.dequeue(
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]                      ^^^^^^^^^^^
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 616, in dequeue
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     with self.acquire_read(timeout, cancel, indefinite) as buf:
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/usr/lib/python3.11/contextlib.py", line 137, in __enter__
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     return next(self.gen)
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]            ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 531, in acquire_read
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948]     raise RuntimeError("cancelled")
(EngineCore_DP0 pid=30004) ERROR 02-11 08:51:56 [core.py:948] RuntimeError: cancelled
(APIServer pid=29908) ERROR 02-11 08:51:56 [async_llm.py:693] AsyncLLM output_handler failed.
(APIServer pid=29908) ERROR 02-11 08:51:56 [async_llm.py:693] Traceback (most recent call last):
(APIServer pid=29908) ERROR 02-11 08:51:56 [async_llm.py:693]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 649, in output_handler
(APIServer pid=29908) ERROR 02-11 08:51:56 [async_llm.py:693]     outputs = await engine_core.get_output_async()
(APIServer pid=29908) ERROR 02-11 08:51:56 [async_llm.py:693]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=29908) ERROR 02-11 08:51:56 [async_llm.py:693]   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 894, in get_output_async
(APIServer pid=29908) ERROR 02-11 08:51:56 [async_llm.py:693]     raise self._format_exception(outputs) from None
(APIServer pid=29908) ERROR 02-11 08:51:56 [async_llm.py:693] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
(APIServer pid=29908) INFO:     127.0.0.1:53390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(EngineCore_DP0 pid=30004) Process EngineCore_DP0:
(EngineCore_DP0 pid=30004) Traceback (most recent call last):
(EngineCore_DP0 pid=30004)   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=30004)     self.run()
(EngineCore_DP0 pid=30004)   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=30004)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
(EngineCore_DP0 pid=30004)     raise e
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
(EngineCore_DP0 pid=30004)     engine_core.run_busy_loop()
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 966, in run_busy_loop
(EngineCore_DP0 pid=30004)     self._process_engine_step()
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 999, in _process_engine_step
(EngineCore_DP0 pid=30004)     outputs, model_executed = self.step_fn()
(EngineCore_DP0 pid=30004)                               ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 486, in step_with_batch_queue
(EngineCore_DP0 pid=30004)     model_output = future.result()
(EngineCore_DP0 pid=30004)                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 80, in result
(EngineCore_DP0 pid=30004)     return super().result()
(EngineCore_DP0 pid=30004)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004)   File "/usr/lib/python3.11/concurrent/futures/_base.py", line 449, in result
(EngineCore_DP0 pid=30004)     return self.__get_result()
(EngineCore_DP0 pid=30004)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004)   File "/usr/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
(EngineCore_DP0 pid=30004)     raise self._exception
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 84, in wait_for_response
(EngineCore_DP0 pid=30004)     response = self.aggregate(get_response())
(EngineCore_DP0 pid=30004)                               ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 351, in get_response
(EngineCore_DP0 pid=30004)     status, result = mq.dequeue(
(EngineCore_DP0 pid=30004)                      ^^^^^^^^^^^
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 616, in dequeue
(EngineCore_DP0 pid=30004)     with self.acquire_read(timeout, cancel, indefinite) as buf:
(EngineCore_DP0 pid=30004)   File "/usr/lib/python3.11/contextlib.py", line 137, in __enter__
(EngineCore_DP0 pid=30004)     return next(self.gen)
(EngineCore_DP0 pid=30004)            ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=30004)   File "/home/h202403659/LLM-Server/.venv/lib/python3.11/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 531, in acquire_read
(EngineCore_DP0 pid=30004)     raise RuntimeError("cancelled")
(EngineCore_DP0 pid=30004) RuntimeError: cancelled
(APIServer pid=29908) INFO:     Shutting down
(APIServer pid=29908) INFO:     Waiting for application shutdown.
(APIServer pid=29908) INFO:     Application shutdown complete.
(APIServer pid=29908) INFO:     Finished server process [29908]
/usr/lib/python3.11/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.11/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [utils.py:325] 
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [utils.py:325] 
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gemma-3-12b-it'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=37597)[0;0m WARNING 02-11 10:20:40 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=37597)[0;0m INFO 02-11 10:20:40 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=37597)[0;0m WARNING 02-11 10:20:40 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=37597)[0;0m WARNING 02-11 10:20:42 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=37737)[0;0m INFO 02-11 10:20:51 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gemma-3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=37737)[0;0m WARNING 02-11 10:20:51 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-11 10:20:59 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-11 10:20:59 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-11 10:21:01 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:38059 backend=nccl
INFO 02-11 10:21:01 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:38059 backend=nccl
INFO 02-11 10:21:02 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-11 10:21:02 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-11 10:21:02 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-11 10:21:02 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-11 10:21:02 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 02-11 10:21:02 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-11 10:21:02 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(Worker_TP0 pid=37831)[0;0m INFO 02-11 10:21:13 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(Worker_TP1 pid=37832)[0;0m INFO 02-11 10:21:14 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP1 pid=37832)[0;0m INFO 02-11 10:21:14 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP1 pid=37832)[0;0m WARNING 02-11 10:21:14 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP1 pid=37832)[0;0m INFO 02-11 10:21:14 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=37831)[0;0m INFO 02-11 10:21:14 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP0 pid=37831)[0;0m INFO 02-11 10:21:14 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP0 pid=37831)[0;0m WARNING 02-11 10:21:14 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP0 pid=37831)[0;0m INFO 02-11 10:21:14 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP1 pid=37832)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP1 pid=37832)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP1 pid=37832)[0;0m   warnings.warn(
[0;36m(Worker_TP0 pid=37831)[0;0m /home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP0 pid=37831)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP0 pid=37831)[0;0m   warnings.warn(
[0;36m(Worker_TP0 pid=37831)[0;0m INFO 02-11 10:21:17 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=37831)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=37831)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:38,  9.68s/it]
[0;36m(Worker_TP0 pid=37831)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:18<00:27,  9.12s/it]
[0;36m(Worker_TP0 pid=37831)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:27<00:18,  9.31s/it]
[0;36m(Worker_TP0 pid=37831)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:36<00:09,  9.09s/it]
[0;36m(Worker_TP0 pid=37831)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:45<00:00,  9.13s/it]
[0;36m(Worker_TP0 pid=37831)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:45<00:00,  9.18s/it]
[0;36m(Worker_TP0 pid=37831)[0;0m 
[0;36m(Worker_TP0 pid=37831)[0;0m INFO 02-11 10:22:03 [default_loader.py:291] Loading weights took 45.95 seconds
[0;36m(Worker_TP0 pid=37831)[0;0m INFO 02-11 10:22:04 [gpu_model_runner.py:4130] Model loading took 11.97 GiB memory and 49.511187 seconds
[0;36m(Worker_TP1 pid=37832)[0;0m INFO 02-11 10:22:04 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=37831)[0;0m INFO 02-11 10:22:04 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] WorkerProc hit an exception.
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] Traceback (most recent call last):
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     output = func(*args, **kwargs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return func(*args, **kwargs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 322, in determine_available_memory
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     self.model_runner.profile_run()
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4974, in profile_run
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     dummy_encoder_outputs = self.model.embed_multimodal(
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 597, in embed_multimodal
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._process_image_input(image_input)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 588, in _process_image_input
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     image_embeds = self.multi_modal_projector(image_features)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 471, in forward
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/custom_op.py", line 126, in forward
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._forward_method(*args, **kwargs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py", line 351, in forward_cuda
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self.forward_native(x, residual)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py", line 327, in forward_native
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._forward_static_no_residual(
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1211, in codegen_and_compile
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     torch._dynamo.repro.after_aot.save_graph_repro(
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 497, in save_graph_repro
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     generate_compiler_repro_string(
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 362, in generate_compiler_repro_string
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     model_str += _cuda_system_info_comment()
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py", line 265, in _cuda_system_info_comment
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     cuda_version_out = subprocess.check_output(["nvcc", "--version"])
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 421, in check_output
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 503, in run
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     with Popen(*popenargs, **kwargs) as process:
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 971, in __init__
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     self._execute_child(args, executable, preexec_fn, close_fds,
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 1863, in _execute_child
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     raise child_exception_type(errno_num, err_msg, err_filename)
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] torch._inductor.exc.InductorError: PermissionError: [Errno 13] Permission denied: 'nvcc'
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] 
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[0;36m(Worker_TP1 pid=37832)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] 
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] WorkerProc hit an exception.
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] Traceback (most recent call last):
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     output = func(*args, **kwargs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return func(*args, **kwargs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 322, in determine_available_memory
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     self.model_runner.profile_run()
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4974, in profile_run
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     dummy_encoder_outputs = self.model.embed_multimodal(
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 597, in embed_multimodal
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._process_image_input(image_input)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 588, in _process_image_input
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     image_embeds = self.multi_modal_projector(image_features)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 471, in forward
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/custom_op.py", line 126, in forward
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._forward_method(*args, **kwargs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py", line 351, in forward_cuda
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self.forward_native(x, residual)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py", line 327, in forward_native
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return self._forward_static_no_residual(
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1211, in codegen_and_compile
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     torch._dynamo.repro.after_aot.save_graph_repro(
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 497, in save_graph_repro
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     generate_compiler_repro_string(
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 362, in generate_compiler_repro_string
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     model_str += _cuda_system_info_comment()
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py", line 265, in _cuda_system_info_comment
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     cuda_version_out = subprocess.check_output(["nvcc", "--version"])
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 421, in check_output
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 503, in run
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     with Popen(*popenargs, **kwargs) as process:
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 971, in __init__
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     self._execute_child(args, executable, preexec_fn, close_fds,
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 1863, in _execute_child
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852]     raise child_exception_type(errno_num, err_msg, err_filename)
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] torch._inductor.exc.InductorError: PermissionError: [Errno 13] Permission denied: 'nvcc'
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] 
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[0;36m(Worker_TP0 pid=37831)[0;0m ERROR 02-11 10:22:07 [multiproc_executor.py:852] 
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]     return aggregate(get_response())
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946]     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946] RuntimeError: Worker failed with error 'PermissionError: [Errno 13] Permission denied: 'nvcc'
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946] 
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:07 [core.py:946] ', please check the stack trace above for the root cause
[0;36m(Worker_TP1 pid=37832)[0;0m WARNING 02-11 10:22:07 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(Worker_TP0 pid=37831)[0;0m WARNING 02-11 10:22:07 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(EngineCore_DP0 pid=37737)[0;0m ERROR 02-11 10:22:09 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[0;36m(EngineCore_DP0 pid=37737)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=37737)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=37737)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=37737)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=37737)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=37737)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=37737)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=37737)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=37737)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=37737)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[0;36m(EngineCore_DP0 pid=37737)[0;0m     return aggregate(get_response())
[0;36m(EngineCore_DP0 pid=37737)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[0;36m(EngineCore_DP0 pid=37737)[0;0m     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=37737)[0;0m RuntimeError: Worker failed with error 'PermissionError: [Errno 13] Permission denied: 'nvcc'
[0;36m(EngineCore_DP0 pid=37737)[0;0m 
[0;36m(EngineCore_DP0 pid=37737)[0;0m Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[0;36m(EngineCore_DP0 pid=37737)[0;0m ', please check the stack trace above for the root cause
[0;36m(APIServer pid=37597)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=37597)[0;0m   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[0;36m(APIServer pid=37597)[0;0m     return _run_code(code, main_globals, None,
[0;36m(APIServer pid=37597)[0;0m   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[0;36m(APIServer pid=37597)[0;0m     exec(code, run_globals)
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 991, in <module>
[0;36m(APIServer pid=37597)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 69, in run
[0;36m(APIServer pid=37597)[0;0m     return loop.run_until_complete(wrapper())
[0;36m(APIServer pid=37597)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=37597)[0;0m     return await main
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=37597)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=37597)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=37597)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=37597)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=37597)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=37597)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=37597)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=37597)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=37597)[0;0m     return cls(
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=37597)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=37597)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=37597)[0;0m     super().__init__(
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=37597)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=37597)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
[0;36m(APIServer pid=37597)[0;0m     next(self.gen)
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=37597)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=37597)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=37597)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=37597)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [utils.py:325] 
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [utils.py:325] 
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gemma-3-12b-it'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=38564)[0;0m WARNING 02-11 10:22:36 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=38564)[0;0m INFO 02-11 10:22:36 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=38564)[0;0m WARNING 02-11 10:22:36 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=38564)[0;0m WARNING 02-11 10:22:38 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=38687)[0;0m INFO 02-11 10:22:48 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gemma-3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=38687)[0;0m WARNING 02-11 10:22:48 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-11 10:22:55 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-11 10:22:56 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-11 10:22:57 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:33631 backend=nccl
INFO 02-11 10:22:58 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:33631 backend=nccl
INFO 02-11 10:22:58 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-11 10:22:58 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-11 10:22:58 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-11 10:22:58 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-11 10:22:58 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 02-11 10:22:58 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-11 10:22:58 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(Worker_TP0 pid=38781)[0;0m INFO 02-11 10:23:09 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(Worker_TP1 pid=38782)[0;0m INFO 02-11 10:23:09 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP0 pid=38781)[0;0m INFO 02-11 10:23:09 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP1 pid=38782)[0;0m INFO 02-11 10:23:09 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP1 pid=38782)[0;0m WARNING 02-11 10:23:09 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP1 pid=38782)[0;0m INFO 02-11 10:23:09 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=38781)[0;0m INFO 02-11 10:23:09 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP0 pid=38781)[0;0m WARNING 02-11 10:23:09 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP0 pid=38781)[0;0m INFO 02-11 10:23:09 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=38781)[0;0m INFO 02-11 10:23:09 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=38781)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=38781)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.45it/s]
[0;36m(Worker_TP0 pid=38781)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.33it/s]
[0;36m(Worker_TP0 pid=38781)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.29it/s]
[0;36m(Worker_TP0 pid=38781)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.37it/s]
[0;36m(Worker_TP0 pid=38781)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.37it/s]
[0;36m(Worker_TP0 pid=38781)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.36it/s]
[0;36m(Worker_TP0 pid=38781)[0;0m 
[0;36m(Worker_TP0 pid=38781)[0;0m INFO 02-11 10:23:14 [default_loader.py:291] Loading weights took 3.73 seconds
[0;36m(Worker_TP0 pid=38781)[0;0m INFO 02-11 10:23:14 [gpu_model_runner.py:4130] Model loading took 11.97 GiB memory and 5.093179 seconds
[0;36m(Worker_TP0 pid=38781)[0;0m INFO 02-11 10:23:15 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
[0;36m(Worker_TP1 pid=38782)[0;0m INFO 02-11 10:23:15 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852] WorkerProc hit an exception.
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852] Traceback (most recent call last):
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     output = func(*args, **kwargs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return func(*args, **kwargs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 322, in determine_available_memory
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     self.model_runner.profile_run()
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4974, in profile_run
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     dummy_encoder_outputs = self.model.embed_multimodal(
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 597, in embed_multimodal
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return self._process_image_input(image_input)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 588, in _process_image_input
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     image_embeds = self.multi_modal_projector(image_features)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/models/gemma3_mm.py", line 471, in forward
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/custom_op.py", line 126, in forward
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return self._forward_method(*args, **kwargs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py", line 351, in forward_cuda
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return self.forward_native(x, residual)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py", line 327, in forward_native
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return self._forward_static_no_residual(
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1211, in codegen_and_compile
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     torch._dynamo.repro.after_aot.save_graph_repro(
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 497, in save_graph_repro
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     generate_compiler_repro_string(
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 362, in generate_compiler_repro_string
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     model_str += _cuda_system_info_comment()
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py", line 265, in _cuda_system_info_comment
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     cuda_version_out = subprocess.check_output(["nvcc", "--version"])
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 421, in check_output
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 503, in run
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     with Popen(*popenargs, **kwargs) as process:
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 971, in __init__
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     self._execute_child(args, executable, preexec_fn, close_fds,
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]   File "/usr/lib/python3.10/subprocess.py", line 1863, in _execute_child
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852]     raise child_exception_type(errno_num, err_msg, err_filename)
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852] torch._inductor.exc.InductorError: PermissionError: [Errno 13] Permission denied: 'nvcc'
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852] 
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[0;36m(Worker_TP0 pid=38781)[0;0m ERROR 02-11 10:23:18 [multiproc_executor.py:852] 
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]     return aggregate(get_response())
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946]     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946] RuntimeError: Worker failed with error 'PermissionError: [Errno 13] Permission denied: 'nvcc'
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946] 
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:18 [core.py:946] ', please check the stack trace above for the root cause
[0;36m(Worker_TP0 pid=38781)[0;0m WARNING 02-11 10:23:18 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(Worker_TP1 pid=38782)[0;0m WARNING 02-11 10:23:18 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(EngineCore_DP0 pid=38687)[0;0m ERROR 02-11 10:23:20 [multiproc_executor.py:246] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
[0;36m(EngineCore_DP0 pid=38687)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=38687)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=38687)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=38687)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=38687)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=38687)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=38687)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[0;36m(EngineCore_DP0 pid=38687)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=38687)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=38687)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[0;36m(EngineCore_DP0 pid=38687)[0;0m     return aggregate(get_response())
[0;36m(EngineCore_DP0 pid=38687)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[0;36m(EngineCore_DP0 pid=38687)[0;0m     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=38687)[0;0m RuntimeError: Worker failed with error 'PermissionError: [Errno 13] Permission denied: 'nvcc'
[0;36m(EngineCore_DP0 pid=38687)[0;0m 
[0;36m(EngineCore_DP0 pid=38687)[0;0m Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[0;36m(EngineCore_DP0 pid=38687)[0;0m ', please check the stack trace above for the root cause
[0;36m(APIServer pid=38564)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=38564)[0;0m   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[0;36m(APIServer pid=38564)[0;0m     return _run_code(code, main_globals, None,
[0;36m(APIServer pid=38564)[0;0m   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[0;36m(APIServer pid=38564)[0;0m     exec(code, run_globals)
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 991, in <module>
[0;36m(APIServer pid=38564)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 69, in run
[0;36m(APIServer pid=38564)[0;0m     return loop.run_until_complete(wrapper())
[0;36m(APIServer pid=38564)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=38564)[0;0m     return await main
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=38564)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=38564)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=38564)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=38564)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=38564)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=38564)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=38564)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=38564)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=38564)[0;0m     return cls(
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=38564)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=38564)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=38564)[0;0m     super().__init__(
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=38564)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=38564)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
[0;36m(APIServer pid=38564)[0;0m     next(self.gen)
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=38564)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=38564)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=38564)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=38564)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [utils.py:325] 
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gemma3-12b-it
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [utils.py:325] 
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gemma-3-12b-it'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8, 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [model.py:541] Resolved architecture: Gemma3ForConditionalGeneration
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:16 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:17 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:17 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=39257)[0;0m WARNING 02-11 10:24:17 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:24:17 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=39257)[0;0m WARNING 02-11 10:24:17 [cuda.py:257] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[0;36m(APIServer pid=39257)[0;0m WARNING 02-11 10:24:17 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=39380)[0;0m INFO 02-11 10:24:26 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gemma3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gemma-3-12b-it, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=39380)[0;0m WARNING 02-11 10:24:26 [multiproc_executor.py:910] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 02-11 10:24:32 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 02-11 10:24:32 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 02-11 10:24:34 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:39751 backend=nccl
INFO 02-11 10:24:34 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:39751 backend=nccl
INFO 02-11 10:24:34 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-11 10:24:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-11 10:24:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 02-11 10:24:35 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-11 10:24:35 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 02-11 10:24:35 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-11 10:24:35 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:45 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gemma3-12b-it...
[0;36m(Worker_TP1 pid=39471)[0;0m INFO 02-11 10:24:45 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP1 pid=39471)[0;0m INFO 02-11 10:24:45 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP1 pid=39471)[0;0m WARNING 02-11 10:24:45 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP1 pid=39471)[0;0m INFO 02-11 10:24:45 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:45 [mm_encoder_attention.py:77] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:45 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(Worker_TP0 pid=39470)[0;0m WARNING 02-11 10:24:45 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:45 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:46 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=39470)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=39470)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:03,  1.24it/s]
[0;36m(Worker_TP0 pid=39470)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.16it/s]
[0;36m(Worker_TP0 pid=39470)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.12it/s]
[0;36m(Worker_TP0 pid=39470)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:03<00:00,  1.11it/s]
[0;36m(Worker_TP0 pid=39470)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:04<00:00,  1.17it/s]
[0;36m(Worker_TP0 pid=39470)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:04<00:00,  1.16it/s]
[0;36m(Worker_TP0 pid=39470)[0;0m 
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:51 [default_loader.py:291] Loading weights took 4.37 seconds
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:51 [gpu_model_runner.py:4130] Model loading took 11.97 GiB memory and 5.566713 seconds
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:51 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
[0;36m(Worker_TP1 pid=39471)[0;0m INFO 02-11 10:24:51 [gpu_model_runner.py:4958] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 8 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=39470)[0;0m INFO 02-11 10:24:55 [gpu_worker.py:356] Available KV cache memory: 6.6 GiB
[0;36m(EngineCore_DP0 pid=39380)[0;0m INFO 02-11 10:24:55 [kv_cache_utils.py:1307] GPU KV cache size: 36,064 tokens
[0;36m(EngineCore_DP0 pid=39380)[0;0m INFO 02-11 10:24:55 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 9.16x
[0;36m(EngineCore_DP0 pid=39380)[0;0m INFO 02-11 10:24:57 [core.py:272] init engine (profile, create kv cache, warmup model) took 5.47 seconds
[0;36m(EngineCore_DP0 pid=39380)[0;0m WARNING 02-11 10:24:59 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=39380)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(EngineCore_DP0 pid=39380)[0;0m INFO 02-11 10:25:09 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=39380)[0;0m WARNING 02-11 10:25:09 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=39380)[0;0m INFO 02-11 10:25:09 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:09 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=39257)[0;0m WARNING 02-11 10:25:09 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_k': 64, 'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:09 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=39257)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [hf.py:310] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [serving.py:212] Chat template warmup completed in 2631.2ms
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:25:12 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=39257)[0;0m INFO:     Started server process [39257]
[0;36m(APIServer pid=39257)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=39257)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:25:29 [serving.py:231] Error with model error=ErrorInfo(message='The model `/home/h202403659/LLM-Server/models/llm/gemma3-12b-it` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:62527 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:25:49 [serving.py:231] Error with model error=ErrorInfo(message='The model `gemma3-12b-it` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:62572 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:26:06 [serving.py:231] Error with model error=ErrorInfo(message='The model `/home/h202403659/LLM-Server/models/llm` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:62595 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:27:12 [serving.py:231] Error with model error=ErrorInfo(message='The model `/home/h202403659/LLM-Server/models/llm` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54312 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:27:13 [serving.py:231] Error with model error=ErrorInfo(message='The model `/home/h202403659/LLM-Server/models/llm` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54312 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:27:21 [serving.py:231] Error with model error=ErrorInfo(message='The model `/home/h202403659/LLM-Server/models/llm` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54323 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:27:33 [serving.py:231] Error with model error=ErrorInfo(message='The model `gemma3-12b-it` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54336 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:27:34 [serving.py:231] Error with model error=ErrorInfo(message='The model `gemma3-12b-it` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54336 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:27:35 [serving.py:231] Error with model error=ErrorInfo(message='The model `gemma3-12b-it` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54336 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54383 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 10:29:06 [serving.py:231] Error with model error=ErrorInfo(message='The model `gemma3-12b-it` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54479 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:30:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:30:12 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:30:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:54539 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:30:32 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 10:30:42 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:01:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 47.1%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:57776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:01:32 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 47.1%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:01:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 62.7%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:57806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:01:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.7%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:02:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.7%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:07:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 3.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:07:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:53821 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:07:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:07:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:11:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 75.3%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:56618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:11:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 75.3%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:11:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 75.3%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:13:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 78.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:56853 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:13:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 78.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:13:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 78.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:14:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 67.2%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:14:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 67.2%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:14:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 67.2%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:14:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 67.2%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:56903 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:14:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 67.2%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 11:14:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 67.2%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:46:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:46:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:46:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:62870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:46:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:46:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 70.6%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:47:53 [loggers.py:257] Engine 000: Avg prompt throughput: 2.3 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 60.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:48:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 60.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:48:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 60.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:48:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 60.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:28 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 8196. This model's maximum context length is 8192 tokens and your request has 23 input tokens (8196 > 8192 - 23). (parameter=max_tokens, value=8196)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:63083 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:48:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323] Error in preprocessing prompt inputs
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323] Traceback (most recent call last):
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323]     raise VLLMValidationError(
[0;36m(APIServer pid=39257)[0;0m ERROR 02-11 12:48:35 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 8192 tokens and your request has 23 input tokens (8192 > 8192 - 23). (parameter=max_tokens, value=8192)
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:63093 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:48:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.4%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:48:53 [loggers.py:257] Engine 000: Avg prompt throughput: 2.3 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:49:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:49:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:49:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:49:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:49:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:49:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:50:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:50:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:50:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:50:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:50:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:50:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:51:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:51:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:51:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:51:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:51:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:51:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:52:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:52:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:52:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:52:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:52:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:52:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO:     172.18.176.1:63120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:53:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=39257)[0;0m INFO 02-11 12:53:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.5%, MM cache hit rate: 0.0%
/usr/bin/python: Error while finding module specification for 'vllm.entrypoints.openai.api_server' (ModuleNotFoundError: No module named 'vllm')
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:12 [utils.py:325] 
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:12 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:12 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:12 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gpt-oss-20b
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:12 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:12 [utils.py:325] 
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:12 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gpt-oss-20b'], 'gpu_memory_utilization': 0.95, 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:18 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=268262)[0;0m ERROR 02-11 16:45:18 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=268262)[0;0m ERROR 02-11 16:45:20 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:20 [model.py:1882] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:20 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:20 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:20 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:20 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=268262)[0;0m WARNING 02-11 16:45:20 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=268262)[0;0m INFO 02-11 16:45:20 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=268262)[0;0m WARNING 02-11 16:45:22 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=268368)[0;0m INFO 02-11 16:45:28 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=268368)[0;0m INFO 02-11 16:45:28 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:44153 backend=nccl
[0;36m(EngineCore_DP0 pid=268368)[0;0m INFO 02-11 16:45:28 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946]     raise ValueError(
[0;36m(EngineCore_DP0 pid=268368)[0;0m ERROR 02-11 16:45:29 [core.py:946] ValueError: Free memory on device cuda:0 (22.7/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[0;36m(EngineCore_DP0 pid=268368)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=268368)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=268368)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=268368)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=268368)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=268368)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=268368)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=268368)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=268368)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=268368)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=268368)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=268368)[0;0m     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=268368)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=268368)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=268368)[0;0m ValueError: Free memory on device cuda:0 (22.7/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W211 16:45:29.767133539 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=268262)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=268262)[0;0m   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[0;36m(APIServer pid=268262)[0;0m     return _run_code(code, main_globals, None,
[0;36m(APIServer pid=268262)[0;0m   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[0;36m(APIServer pid=268262)[0;0m     exec(code, run_globals)
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 991, in <module>
[0;36m(APIServer pid=268262)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 69, in run
[0;36m(APIServer pid=268262)[0;0m     return loop.run_until_complete(wrapper())
[0;36m(APIServer pid=268262)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=268262)[0;0m     return await main
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=268262)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=268262)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=268262)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=268262)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=268262)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=268262)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=268262)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=268262)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=268262)[0;0m     return cls(
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=268262)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=268262)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=268262)[0;0m     super().__init__(
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=268262)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=268262)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
[0;36m(APIServer pid=268262)[0;0m     next(self.gen)
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=268262)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=268262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=268262)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=268262)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:27 [utils.py:325] 
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:27 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:27 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:27 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gpt-oss-20b
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:27 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:27 [utils.py:325] 
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:27 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gpt-oss-20b'], 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:27 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=268493)[0;0m ERROR 02-11 16:46:27 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=268493)[0;0m ERROR 02-11 16:46:29 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:29 [model.py:1882] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:29 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:30 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:30 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:30 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=268493)[0;0m WARNING 02-11 16:46:30 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:30 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=268493)[0;0m WARNING 02-11 16:46:31 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:37 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:38 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:37961 backend=nccl
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:38 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=268566)[0;0m WARNING 02-11 16:46:38 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:38 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gpt-oss-20b...
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:39 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN',)
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:39 [mxfp4.py:161] Using Marlin backend
[0;36m(EngineCore_DP0 pid=268566)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=268566)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.40s/it]
[0;36m(EngineCore_DP0 pid=268566)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:03<00:01,  1.62s/it]
[0;36m(EngineCore_DP0 pid=268566)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:05<00:00,  1.76s/it]
[0;36m(EngineCore_DP0 pid=268566)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:05<00:00,  1.70s/it]
[0;36m(EngineCore_DP0 pid=268566)[0;0m 
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:45 [default_loader.py:291] Loading weights took 5.22 seconds
[0;36m(EngineCore_DP0 pid=268566)[0;0m WARNING 02-11 16:46:45 [marlin_utils_fp4.py:336] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:46 [gpu_model_runner.py:4130] Model loading took 13.72 GiB memory and 7.415886 seconds
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:48 [gpu_worker.py:356] Available KV cache memory: 7.65 GiB
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:48 [kv_cache_utils.py:1307] GPU KV cache size: 167,072 tokens
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:48 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 32.18x
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:51 [core.py:272] init engine (profile, create kv cache, warmup model) took 5.09 seconds
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:53 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=268566)[0;0m WARNING 02-11 16:46:53 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=268566)[0;0m INFO 02-11 16:46:53 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:53 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=268493)[0;0m WARNING 02-11 16:46:53 [serving.py:242] For gpt-oss, we ignore --enable-auto-tool-choice and always enable tool use.
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:57 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [serving.py:212] Chat template warmup completed in 1095.2ms
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:46:58 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=268493)[0;0m INFO:     Started server process [268493]
[0;36m(APIServer pid=268493)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=268493)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=268493)[0;0m ERROR 02-11 16:49:56 [serving.py:231] Error with model error=ErrorInfo(message='The model `gemma-3-12b-it` does not exist.', type='NotFoundError', param='model', code=404)
[0;36m(APIServer pid=268493)[0;0m INFO:     172.18.176.1:58669 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=268493)[0;0m INFO:     172.18.176.1:64104 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=268493)[0;0m INFO:     172.18.176.1:64104 - "GET /favicon.ico HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=268493)[0;0m INFO:     172.18.176.1:57426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:50:38 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:50:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268493)[0;0m INFO:     172.18.176.1:57450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:50:58 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 47.8%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:51:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 47.8%
[0;36m(APIServer pid=268493)[0;0m INFO:     172.18.176.1:57492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:51:38 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.7%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:51:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.7%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:51:58 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 10.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 63.5%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:52:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 63.5%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:52:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 63.5%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:52:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 63.5%
[0;36m(APIServer pid=268493)[0;0m INFO:     172.18.176.1:57514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:52:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.5%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:52:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.5%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:53:48 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 68.0%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:53:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 68.0%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:54:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 68.0%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:54:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 68.0%
[0;36m(APIServer pid=268493)[0;0m INFO:     172.18.176.1:57609 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:54:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 68.0%
[0;36m(APIServer pid=268493)[0;0m INFO 02-11 16:54:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 68.0%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:09 [utils.py:325] 
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:09 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:09 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:09 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gpt-oss-20b
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:09 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:09 [utils.py:325] 
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:09 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gpt-oss-20b'], 'swap_space': 8.0, 'enable_prefix_caching': True, 'max_num_seqs': 16}
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:09 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=268904)[0;0m ERROR 02-11 16:56:09 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=268904)[0;0m ERROR 02-11 16:56:11 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:11 [model.py:1882] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:11 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:12 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:12 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:12 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=268904)[0;0m WARNING 02-11 16:56:12 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:12 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=268904)[0;0m WARNING 02-11 16:56:13 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:19 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:20 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:44889 backend=nccl
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:20 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=268977)[0;0m WARNING 02-11 16:56:20 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:20 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gpt-oss-20b...
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:21 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN',)
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:21 [mxfp4.py:161] Using Marlin backend
[0;36m(EngineCore_DP0 pid=268977)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=268977)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.27s/it]
[0;36m(EngineCore_DP0 pid=268977)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.43s/it]
[0;36m(EngineCore_DP0 pid=268977)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.55s/it]
[0;36m(EngineCore_DP0 pid=268977)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.50s/it]
[0;36m(EngineCore_DP0 pid=268977)[0;0m 
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:26 [default_loader.py:291] Loading weights took 4.81 seconds
[0;36m(EngineCore_DP0 pid=268977)[0;0m WARNING 02-11 16:56:26 [marlin_utils_fp4.py:336] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:27 [gpu_model_runner.py:4130] Model loading took 13.72 GiB memory and 6.831531 seconds
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:30 [gpu_worker.py:356] Available KV cache memory: 7.65 GiB
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:30 [kv_cache_utils.py:1307] GPU KV cache size: 167,072 tokens
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:30 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 32.18x
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:30 [core.py:272] init engine (profile, create kv cache, warmup model) took 2.78 seconds
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:33 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=268977)[0;0m WARNING 02-11 16:56:33 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=268977)[0;0m INFO 02-11 16:56:33 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:33 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=268904)[0;0m WARNING 02-11 16:56:33 [serving.py:242] For gpt-oss, we ignore --enable-auto-tool-choice and always enable tool use.
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:33 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [serving.py:212] Chat template warmup completed in 1468.8ms
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:56:35 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=268904)[0;0m INFO:     Started server process [268904]
[0;36m(APIServer pid=268904)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=268904)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:57:05 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:57:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:57:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:57:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:57:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268904)[0;0m INFO:     172.18.176.1:59750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:57:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 16:58:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 17:03:05 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 42.1%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 17:03:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 42.1%
[0;36m(APIServer pid=268904)[0;0m INFO 02-11 17:03:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 42.1%
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:54 [utils.py:325] 
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:54 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:54 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:54 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gpt-oss-20b
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:54 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:54 [utils.py:325] 
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:54 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gpt-oss-20b'], 'swap_space': 8.0, 'enable_prefix_caching': True, 'max_num_batched_tokens': 8192, 'max_num_seqs': 256, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:54 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=269262)[0;0m ERROR 02-11 18:05:54 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=269262)[0;0m ERROR 02-11 18:05:57 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:57 [model.py:1882] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:57 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:58 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:58 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:58 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=269262)[0;0m WARNING 02-11 18:05:58 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=269262)[0;0m INFO 02-11 18:05:58 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=269262)[0;0m WARNING 02-11 18:06:01 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=269335)[0;0m INFO 02-11 18:06:10 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=269335)[0;0m INFO 02-11 18:06:11 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:55409 backend=nccl
[0;36m(EngineCore_DP0 pid=269335)[0;0m INFO 02-11 18:06:11 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946]     raise ValueError(
[0;36m(EngineCore_DP0 pid=269335)[0;0m ERROR 02-11 18:06:11 [core.py:946] ValueError: Free memory on device cuda:0 (11.17/23.99 GiB) on startup is less than desired GPU memory utilization (0.9, 21.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[0;36m(EngineCore_DP0 pid=269335)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=269335)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=269335)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=269335)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=269335)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=269335)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=269335)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=269335)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=269335)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=269335)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=269335)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=269335)[0;0m     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=269335)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=269335)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=269335)[0;0m ValueError: Free memory on device cuda:0 (11.17/23.99 GiB) on startup is less than desired GPU memory utilization (0.9, 21.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W211 18:06:11.658361849 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=269262)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=269262)[0;0m   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[0;36m(APIServer pid=269262)[0;0m     return _run_code(code, main_globals, None,
[0;36m(APIServer pid=269262)[0;0m   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[0;36m(APIServer pid=269262)[0;0m     exec(code, run_globals)
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 991, in <module>
[0;36m(APIServer pid=269262)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 69, in run
[0;36m(APIServer pid=269262)[0;0m     return loop.run_until_complete(wrapper())
[0;36m(APIServer pid=269262)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=269262)[0;0m     return await main
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=269262)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=269262)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=269262)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=269262)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=269262)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=269262)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=269262)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=269262)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=269262)[0;0m     return cls(
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=269262)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=269262)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=269262)[0;0m     super().__init__(
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=269262)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=269262)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
[0;36m(APIServer pid=269262)[0;0m     next(self.gen)
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=269262)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=269262)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=269262)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=269262)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:29 [utils.py:325] 
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:29 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:29 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:29 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gpt-oss-20b
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:29 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:29 [utils.py:325] 
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:29 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gpt-oss-20b'], 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:29 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=269493)[0;0m ERROR 02-11 18:13:29 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=269493)[0;0m ERROR 02-11 18:13:31 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:31 [model.py:1882] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:31 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:32 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:32 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:32 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=269493)[0;0m WARNING 02-11 18:13:32 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=269493)[0;0m INFO 02-11 18:13:32 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=269493)[0;0m WARNING 02-11 18:13:34 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=269566)[0;0m INFO 02-11 18:13:40 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=269566)[0;0m INFO 02-11 18:13:40 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:55511 backend=nccl
[0;36m(EngineCore_DP0 pid=269566)[0;0m INFO 02-11 18:13:40 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946]     raise ValueError(
[0;36m(EngineCore_DP0 pid=269566)[0;0m ERROR 02-11 18:13:41 [core.py:946] ValueError: Free memory on device cuda:0 (11.17/23.99 GiB) on startup is less than desired GPU memory utilization (0.9, 21.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[0;36m(EngineCore_DP0 pid=269566)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=269566)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=269566)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=269566)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=269566)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=269566)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=269566)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=269566)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=269566)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=269566)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=269566)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=269566)[0;0m     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=269566)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=269566)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=269566)[0;0m ValueError: Free memory on device cuda:0 (11.17/23.99 GiB) on startup is less than desired GPU memory utilization (0.9, 21.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W211 18:13:41.304767315 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=269493)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=269493)[0;0m   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[0;36m(APIServer pid=269493)[0;0m     return _run_code(code, main_globals, None,
[0;36m(APIServer pid=269493)[0;0m   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[0;36m(APIServer pid=269493)[0;0m     exec(code, run_globals)
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 991, in <module>
[0;36m(APIServer pid=269493)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 69, in run
[0;36m(APIServer pid=269493)[0;0m     return loop.run_until_complete(wrapper())
[0;36m(APIServer pid=269493)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=269493)[0;0m     return await main
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=269493)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=269493)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=269493)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=269493)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=269493)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=269493)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=269493)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=269493)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=269493)[0;0m     return cls(
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=269493)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=269493)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=269493)[0;0m     super().__init__(
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=269493)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=269493)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
[0;36m(APIServer pid=269493)[0;0m     next(self.gen)
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=269493)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=269493)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=269493)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=269493)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:57 [utils.py:325] 
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:57 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:57 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:57 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gpt-oss-20b
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:57 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:57 [utils.py:325] 
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:57 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gpt-oss-20b'], 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:57 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=270682)[0;0m ERROR 02-12 08:47:57 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=270682)[0;0m ERROR 02-12 08:47:59 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:59 [model.py:1882] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:59 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:59 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:59 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:59 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=270682)[0;0m WARNING 02-12 08:47:59 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=270682)[0;0m INFO 02-12 08:47:59 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=270682)[0;0m WARNING 02-12 08:48:00 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=270755)[0;0m INFO 02-12 08:48:07 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=270755)[0;0m INFO 02-12 08:48:08 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:41497 backend=nccl
[0;36m(EngineCore_DP0 pid=270755)[0;0m INFO 02-12 08:48:08 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946]     raise ValueError(
[0;36m(EngineCore_DP0 pid=270755)[0;0m ERROR 02-12 08:48:08 [core.py:946] ValueError: Free memory on device cuda:0 (11.17/23.99 GiB) on startup is less than desired GPU memory utilization (0.9, 21.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[0;36m(EngineCore_DP0 pid=270755)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=270755)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=270755)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=270755)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=270755)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=270755)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=270755)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=270755)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=270755)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=270755)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=270755)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=270755)[0;0m     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=270755)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=270755)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=270755)[0;0m ValueError: Free memory on device cuda:0 (11.17/23.99 GiB) on startup is less than desired GPU memory utilization (0.9, 21.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W212 08:48:08.525564773 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=270682)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=270682)[0;0m   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[0;36m(APIServer pid=270682)[0;0m     return _run_code(code, main_globals, None,
[0;36m(APIServer pid=270682)[0;0m   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[0;36m(APIServer pid=270682)[0;0m     exec(code, run_globals)
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 991, in <module>
[0;36m(APIServer pid=270682)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 69, in run
[0;36m(APIServer pid=270682)[0;0m     return loop.run_until_complete(wrapper())
[0;36m(APIServer pid=270682)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=270682)[0;0m     return await main
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=270682)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=270682)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=270682)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=270682)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=270682)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=270682)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=270682)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=270682)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=270682)[0;0m     return cls(
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=270682)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=270682)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=270682)[0;0m     super().__init__(
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=270682)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=270682)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
[0;36m(APIServer pid=270682)[0;0m     next(self.gen)
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=270682)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=270682)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=270682)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=270682)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:14 [utils.py:325] 
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:14 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:14 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:14 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gpt-oss-20b
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:14 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:14 [utils.py:325] 
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:14 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gpt-oss-20b'], 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:14 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=270895)[0;0m ERROR 02-12 08:50:14 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=270895)[0;0m ERROR 02-12 08:50:17 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:17 [model.py:1882] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:17 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:18 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:18 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:18 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=270895)[0;0m WARNING 02-12 08:50:18 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=270895)[0;0m INFO 02-12 08:50:18 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=270895)[0;0m WARNING 02-12 08:50:19 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=270968)[0;0m INFO 02-12 08:50:25 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=270968)[0;0m INFO 02-12 08:50:26 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:33269 backend=nccl
[0;36m(EngineCore_DP0 pid=270968)[0;0m INFO 02-12 08:50:26 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946]     raise ValueError(
[0;36m(EngineCore_DP0 pid=270968)[0;0m ERROR 02-12 08:50:26 [core.py:946] ValueError: Free memory on device cuda:0 (11.17/23.99 GiB) on startup is less than desired GPU memory utilization (0.9, 21.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[0;36m(EngineCore_DP0 pid=270968)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=270968)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=270968)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=270968)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=270968)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=270968)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=270968)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=270968)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=270968)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=270968)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=270968)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=270968)[0;0m     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=270968)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=270968)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=270968)[0;0m ValueError: Free memory on device cuda:0 (11.17/23.99 GiB) on startup is less than desired GPU memory utilization (0.9, 21.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W212 08:50:26.367009661 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=270895)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=270895)[0;0m   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[0;36m(APIServer pid=270895)[0;0m     return _run_code(code, main_globals, None,
[0;36m(APIServer pid=270895)[0;0m   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[0;36m(APIServer pid=270895)[0;0m     exec(code, run_globals)
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 991, in <module>
[0;36m(APIServer pid=270895)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 69, in run
[0;36m(APIServer pid=270895)[0;0m     return loop.run_until_complete(wrapper())
[0;36m(APIServer pid=270895)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=270895)[0;0m     return await main
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=270895)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=270895)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=270895)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=270895)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=270895)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=270895)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
[0;36m(APIServer pid=270895)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=270895)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=270895)[0;0m     return cls(
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=270895)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=270895)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=270895)[0;0m     super().__init__(
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=270895)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=270895)[0;0m   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
[0;36m(APIServer pid=270895)[0;0m     next(self.gen)
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=270895)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=270895)[0;0m   File "/home/h202403659/LLM-Server/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=270895)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=270895)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:04 [utils.py:325] 
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:04 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:04 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:04 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /home/h202403659/LLM-Server/models/llm/gpt-oss-20b
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:04 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:04 [utils.py:325] 
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:04 [utils.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'tokenizer': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', 'max_model_len': 8192, 'enforce_eager': True, 'served_model_name': ['gpt-oss-20b'], 'swap_space': 8.0, 'max_num_seqs': 16}
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:04 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=271252)[0;0m ERROR 02-12 08:53:04 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=271252)[0;0m ERROR 02-12 08:53:06 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/h202403659/LLM-Server/models/llm/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:06 [model.py:1882] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:06 [model.py:1561] Using max model len 8192
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:07 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:07 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:07 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=271252)[0;0m WARNING 02-12 08:53:07 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:07 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=271252)[0;0m WARNING 02-12 08:53:08 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:14 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', speculative_config=None, tokenizer='/home/h202403659/LLM-Server/models/llm/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:14 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.181.51:47527 backend=nccl
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:15 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=271325)[0;0m WARNING 02-12 08:53:15 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:15 [gpu_model_runner.py:4033] Starting to load model /home/h202403659/LLM-Server/models/llm/gpt-oss-20b...
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:16 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN',)
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:16 [mxfp4.py:161] Using Marlin backend
[0;36m(EngineCore_DP0 pid=271325)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=271325)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:03,  1.60s/it]
[0;36m(EngineCore_DP0 pid=271325)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:03<00:01,  1.61s/it]
[0;36m(EngineCore_DP0 pid=271325)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:05<00:00,  1.73s/it]
[0;36m(EngineCore_DP0 pid=271325)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:05<00:00,  1.70s/it]
[0;36m(EngineCore_DP0 pid=271325)[0;0m 
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:21 [default_loader.py:291] Loading weights took 5.21 seconds
[0;36m(EngineCore_DP0 pid=271325)[0;0m WARNING 02-12 08:53:21 [marlin_utils_fp4.py:336] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:23 [gpu_model_runner.py:4130] Model loading took 13.72 GiB memory and 7.797852 seconds
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:25 [gpu_worker.py:356] Available KV cache memory: 7.65 GiB
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:25 [kv_cache_utils.py:1307] GPU KV cache size: 167,072 tokens
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:25 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 32.18x
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:25 [core.py:272] init engine (profile, create kv cache, warmup model) took 1.76 seconds
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:26 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=271325)[0;0m WARNING 02-12 08:53:26 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=271325)[0;0m INFO 02-12 08:53:26 [vllm.py:762] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:27 [api_server.py:665] Supported tasks: ['generate']
[0;36m(APIServer pid=271252)[0;0m WARNING 02-12 08:53:27 [serving.py:242] For gpt-oss, we ignore --enable-auto-tool-choice and always enable tool use.
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:27 [serving.py:177] Warming up chat template processing...
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [serving.py:212] Chat template warmup completed in 905.3ms
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO 02-12 08:53:28 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=271252)[0;0m INFO:     Started server process [271252]
[0;36m(APIServer pid=271252)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=271252)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:56613 - "GET /openai HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:56613 - "GET /openai.json HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:49435 - "GET /docs HTTP/1.1" 200 OK
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:49435 - "GET /openapi.json HTTP/1.1" 200 OK
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:59477 - "GET /health HTTP/1.1" 200 OK
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:63283 - "GET /metrics HTTP/1.1" 200 OK
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:59772 - "GET /v1/embeddings HTTP/1.1" 405 Method Not Allowed
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:54880 - "GET /rernk HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:54880 - "GET /rerank HTTP/1.1" 405 Method Not Allowed
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:53158 - "GET /v1/scors HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=271252)[0;0m INFO:     172.18.176.1:53158 - "GET /v1/scores HTTP/1.1" 404 Not Found
